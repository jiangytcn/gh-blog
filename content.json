{"pages":[{"title":"about","text":"2012年开始一直从事云计算开发，OpenStack、CloudStack、CloudFoundry、Kube* 以及现在的安全相关软件、平台服务 时间线 201505-至今 IBM China PaaS cloudfoundry 系统研发 Vault-as-a-Service 服务开发 DevOps 平台开发 201311-201505 神州数码 基于Cloudstack 完成智慧城市私有云的建设主要负责CS各模块的稳定运行，根据功能需求，添加plugin 熟练掌握ACS各模块原理，Maven 项目管理，可进行二次开发 201206-201311 北京海云捷迅科技有限公司 该公司主要发展方向是，基于OpenStack为企业提供开源云计算解决方案该项目现为北京最大的民营IDC提供公有云解决方案，线上成功部署虚拟机近千台任职期间，主要参与开发设计上层用户自服务云计算管理平台，包括前台页面展示、后台数据处理、计费和后期的系统优化及部分运维工作 根据公司前期制定的计划，在经过4个月的时间内，主要在LINUX 平台中使用的Spring，struts 框架， 以RESTAPI 来与底层的openstack 各个服务通信，完成更自服务平台的开发设计，包括监控、LbaaS功能的完成","link":"/about/index.html"}],"posts":[{"title":"Setting Debug Environment Of Cloudstack In Production","text":"This introduction will outline specifically how setting debug environment in production of Cloudstack, and how building a single project into a patch. Configure Tomcat$ vim /usr/sbin/tomcat6 Adding -server -Xdebug -Xnoagent -Djava.compiler=NONE -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8787 into the file above 35 -Djava.io.tmpdir=\"$CATALINA_TMPDIR\" \\ 36 -Djava.util.logging.config.file=\"${CATALINA_BASE}/conf/logging.properties\" \\ 37 -Djava.util.logging.manager=\"org.apache.juli.ClassLoaderLogManager\" \\ 38 -server -Xdebug -Xnoagent -Djava.compiler=NONE -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8787 \\ Now configure you eclipse to the server, go Debugging. Building Single Project Some reasons you modified a project, for instance cloud-server, and you want your new code to be taken into effect, you should build a new jar file, and then applying the patch.cd ~/cloudstack4.1.0 mvn clean mvn -pl :cloud-server After a while when you saw something like below, then you got a new patch. ------------------------------------------------------------------------ BUILD SUCCESS ------------------------------------------------------------------------ Total time: 3:01.915s Finished at: Wed Feb 12 14:56:24 CST 2014 Final Memory: 26M/233M And replace the old cloud-server-4.1.0.jar with new one , restart cloudstack-management, go hacking.","link":"/2014/02/12/setting-debug-environment-of-cloudstack-in-production/"},{"title":"host cpu comprehension","text":"Linux 物理机 查看Linux系统中物理与逻辑cpu的相关信息 Linux 下/proc/cpuinfo文件会显示cpu的信息 逻辑CPU个数是指cat /proc/cpuinfo 所显示的processor的个数 cat /proc/cpuinfo | grep “processor” | wc -l 物理CPU个数，是指physical id（的值）的数量 cat /proc/cpuinfo | grep “physical id” | sort | uniq | wc -l 每个物理CPU中Core的个数：每个相同的physical id都有其对应的core id。如core id分别为1、2、3、4，则表示是Quad-Core CPU，若core id分别是1、2，则表示是Dual-Core。 cat /proc/cpuinfo | grep “cpu cores” | wc -l 逻辑CPU：每个物理CPU中逻辑CPU(可能是core, threads或both)的个数： cat /proc/cpuinfo | grep “siblings” 它既可能是cores的个数，也可能是core的倍数。当它和core的个数相等时，表示每一个core就是一个逻辑CPU，若它时core的2倍时，表示每个core又enable了超线程（Hyper-Thread）。 比如：一个双核的启用了超线程的物理cpu，其core id分别为1、2，但是sibling是4，也就是如果有两个逻辑CPU具有相同的”core id”，那么超线程是打开的。 XenserverXenServer中Socket、Core、以及超线程后的核心之间在XenServer中CPU的排序关系","link":"/2014/03/20/host-cpu-comprehension/"},{"title":"Python RE 正则表达式","text":"####简单介绍正则表达式并不是Python的一部分。正则表达式是用于处理字符串的强大工具，拥有自己独特的语法以及一个独立的处理引擎，效率上可能不如str自带的方法，但功能十分强大。得益于这一点，在提供了正则表达式的语言里，正则表达式的语法都是一样的，区别只在于不同的编程语言实现支持的语法数量不同；但不用担心，不被支持的语法通常是不常用的部分。如果已经在其他语言里使用过正则表达式，只需要简单看一看就可以上手了。下图展示了使用正则表达式进行匹配的流程： 正则表达式的大致匹配过程是：依次拿出表达式和文本中的字符比较，如果每一个字符都能匹配，则匹配成功；一旦有匹配不成功的字符则匹配失败。如果表达式中有量词或边界，这个过程会稍微有一些不同，但也是很好理解的，看下图中的示例以及自己多使用几次就能明白。 下图列出了Python支持的正则表达式元字符和语法 ####数量词的贪婪模式与非贪婪模式正则表达式通常用于在文本中查找匹配的字符串。Python里数量词默认是贪婪的（在少数语言里也可能是默认非贪婪），总是尝试匹配尽可能多的字符；非贪婪的则相反，总是尝试匹配尽可能少的字符。例如：正则表达式”ab*“ 如果用于查找”abbbc”，将找到”abbb”。而如果使用非贪婪的数量词 “ab*?”，将找到”a”。 ####反斜杠的困扰与大多数编程语言相同，正则表达式里使用”&quot;作为转义字符，这就可能造成反斜杠困扰。假如你需要匹配文本中的字符”&quot;，那么使用编程语言表示的正则表达式里将需要4个反斜杠”\\\\“：前两个和后两个分别用于在编程语言里转义成反斜杠，转换成两个反斜杠后再在正则表达式里转义成一个反斜杠。Python里的原生字符串很好地解决了这个问题，这个例子中的正则表达式可以使用r”\\“表示。同样，匹配一个数字的”\\d”可以写成r”\\d”。有了原生字符串，你再也不用担心是不是漏写了反斜杠，写出来的表达式也更直观。 ####匹配模式正则表达式提供了一些可用的匹配模式，比如忽略大小写、多行匹配等，这部分内容将在Pattern类的工厂方法re.compile()中一起介绍。 ###RE 模块 ####开始使用rePython通过re模块提供对正则表达式的支持。使用re的一般步骤是先将正则表达式的字符串形式编译为Pattern实例，然后使用Pattern实例处理文本并获得匹配结果（一个Match实例），最后使用Match实例获得信息，进行其他的操作。 import re # 将正则表达式编译成Pattern对象 pattern = re.compile(r'hello') # 使用Pattern匹配文本，获得匹配结果，无法匹配时将返回None match = pattern.match('hello world!') if match: print match.group() ### 输出 ### # hello re.compile(strPattern [, flag]): 这个方法是Pattern类的工厂方法，用于将字符串形式的正则表达式编译为Pattern对象。 第二个参数flag是匹配模式，取值可以使用按位或运算符’|’表示同时生效，比如re.I | re.M。另外，你也可以在regex字符串中指定模式，比如re.compile(‘pattern’, re.I | re.M)与re.compile(‘(?im)pattern’)是等价的。可选值有： * re.I(re.IGNORECASE): 忽略大小写（括号内是完整写法，下同） * M(MULTILINE): 多行模式，改变&apos;^&apos;和&apos;$&apos;的行为（参见上图） * S(DOTALL): 点任意匹配模式，改变&apos;.&apos;的行为 * L(LOCALE): 使预定字符类 \\w \\W \\b \\B \\s \\S 取决于当前区域设定 * U(UNICODE): 使预定字符类 \\w \\W \\b \\B \\s \\S \\d \\D 取决于unicode定义的字符属性 * X(VERBOSE):详细模式。这个模式下正则表达式可以是多行，忽略空白字符，并可以加入注释。以下两个正则表达式是等价的a = re.compile(r\"\"\" \\d + # the integral part \\. # the decimal point \\d * # some fractional digits\"\"\", re.X) b = re.compile(r\"\\d+\\.\\d*\") re提供了众多模块方法用于完成正则表达式的功能。这些方法可以使用Pattern实例的相应方法替代，唯一的好处是少写一行re.compile()代码，但同时也无法复用编译后的Pattern对象。这些方法将在Pattern类的实例方法部分一起介绍。如上面这个例子可以简写为 m = re.match(r'hello', 'hello world!') print m.group()","link":"/2014/05/15/python-re/"},{"title":"VM Failes to start with error: VDI not available","text":"触发条件: Ssh 至 VM内部，执行关机命令(shutdown -h now), 在NFS backend下的Vm通过Cloudstack无法启动 产生原因：Xenserver 与存储设备或者Lun失去连接 解决方法： Extract VDI UUID of Virtual Machine(VM) that is failing to start from /var/log/cloud/management/management-server.log file For example, VDI UUID will be 6f97582c-xxxx-xxxx-xxxx-9aa5686bcbd36. VM are failing to start with “errorInfo: [SR_BACKEND_FAILURE_46, The VDI is not available [opterr=VDI6f97582c-xxxx-xxxx-xxxx-9aa5686bcbd36 already attached RW]” error Connect to XenServer and make a note of the SR UUID and name-label of the VDI # xe vdi-list uuid= params=sr-uuid, name-label Run the following commands in XenServer # xe vdi-forget uuid= # xe sr-scan uuid= # xe vdi-param-set uuid=< volume extracted from step1 > name-label=< name label extracted from step2 > Restart Cloudstack Management Service","link":"/2014/05/18/vm-failes-to-start-with-error-vdi-not-available/"},{"title":"OpenStack Nova 使用SQLAlchemy 操作Flavor(Mysql backend)","text":"##SQLAlchemy 简介 The SQLAlchemy Object Relational Mapper presents a method of associating user-defined Python classes with database tables, and instances of those classes (objects) with rows in their corresponding tables. It includes a system that transparently synchronizes all changes in state between objects and their related rows, called a unit of work, as well as a system for expressing database queries in terms of the user defined classes and their defined relationships between each other. 我的理解是，SQLAlchemy 是实体/关系映射的一种操作数据的方式， 实体是由Python所编写的类， 这个类的每一个实体，就对应于数据库中的每一条元组（行数据）。其中对这个类的实体的操作，直接映射（影响）到数据库中对应的元组， 这种方式叫做 工作单元（操作单元，有点类似于一个原子操作） 自己写的查询Flavor例子 1 from sqlalchemy.orm import sessionmaker 2 from sqlalchemy import create_engine 3 from sqlalchemy.ext.declarative import declarative_base 4 from sqlalchemy import Column, Integer, String, Float, Boolean 5 6 sql_connection = \"mysql://root:root@localhost/nova?charset=utf8\" 7 engine = create_engine(sql_connection, echo=True) #echo=True 设置该项后，可以打印出sql的执行过程 8 Session = sessionmaker(bind=engine) 9 session = Session() 10 11 class NovaBase(): 12 pass 13 14 BASE = declarative_base() # 用户创建与DB映射的基类 15 16 class InstanceTypes(BASE, NovaBase): 17 __tablename__ = \"instance_types\" # 数据库中表的名称 18 id = Column(Integer, primary_key=True) #Column 用于定义数据表 \"instance_types”的字段， 其参数是该字段的具体的类型 19 name = Column(String(255)) 20 memory_mb = Column(Integer) 21 vcpus = Column(Integer) 22 root_gb = Column(Integer) 23 ephemeral_gb = Column(Integer) 24 flavorid = Column(String(255)) 25 swap = Column(Integer, nullable=False, default=0) 26 rxtx_factor = Column(Float, nullable=False, default=1) 27 vcpu_weight = Column(Integer, nullable=True) 28 disabled = Column(Boolean, default=False) 29 is_public = Column(Boolean, default=True) 30 31 flavors = session.query(InstanceTypes).all() #查询数据表 32 for flavor in flavors: 33 print flavor.id, flavor.name 代码输出 2013-05-15 18:41:10,928 INFO sqlalchemy.engine.base.Engine SELECT DATABASE() 2013-05-15 18:41:10,929 INFO sqlalchemy.engine.base.Engine () 2013-05-15 18:41:10,933 INFO sqlalchemy.engine.base.Engine SHOW VARIABLES LIKE 'character_set%%' 2013-05-15 18:41:10,934 INFO sqlalchemy.engine.base.Engine () 2013-05-15 18:41:10,935 INFO sqlalchemy.engine.base.Engine SHOW VARIABLES LIKE 'lower_case_table_names' 2013-05-15 18:41:10,936 INFO sqlalchemy.engine.base.Engine () 2013-05-15 18:41:10,937 INFO sqlalchemy.engine.base.Engine SHOW COLLATION 2013-05-15 18:41:10,937 INFO sqlalchemy.engine.base.Engine () 2013-05-15 18:41:10,946 INFO sqlalchemy.engine.base.Engine SHOW VARIABLES LIKE 'sql_mode' 2013-05-15 18:41:10,946 INFO sqlalchemy.engine.base.Engine () 2013-05-15 18:41:10,948 INFO sqlalchemy.engine.base.Engine BEGIN (implicit) 2013-05-15 18:41:10,950 INFO sqlalchemy.engine.base.Engine SELECT instance_types.id AS instance_types_id, instance_types.name AS instance_types_name, instance_types.memory_mb AS instance_types_memory_mb, instance_types.vcpus AS instance_types_vcpus, instance_types.root_gb AS instance_types_root_gb, instance_types.ephemeral_gb AS instance_types_ephemeral_gb, instance_types.flavorid AS instance_types_flavorid, instance_types.swap AS instance_types_swap, instance_types.rxtx_factor AS instance_types_rxtx_factor, instance_types.vcpu_weight AS instance_types_vcpu_weight, instance_types.disabled AS instance_types_disabled, instance_types.is_public AS instance_types_is_public FROM instance_types 2013-05-15 18:41:10,950 INFO sqlalchemy.engine.base.Engine () 1 m1.medium 2 m1.tiny 3 m1.large 4 m1.xlarge 5 m1.small 6 m1.nano 7 m1.micro ###Nova 中所对应的代码结构 nova/openstack/common/db/sqlalchemy/models.py 定义了SQLAlchemy 的基类， 所有的对数据的操作，通过该类中定义的方法来实现， 其子类，只需要调用相关的方法即可 nova/db/sqlalchemy/models.py 定义了nova数据中的所有的表,以InstanceTypes 为例 BASE = declarative_base() # 与上一个类子中的方法一样 class NovaBase(models.SoftDeleteMixin, models.ModelBase): #区别在这里， models 为1中定义的模块 pass class InstanceTypes(BASE, NovaBase): __tablename__ = \"instance_types\" # 数据库中表的名称 id = Column(Integer, primary_key=True) #Column 用于定义数据表 \"instance_types”的字段， 其参数是该字段的具体的类型 name = Column(String(255)) memory_mb = Column(Integer) vcpus = Column(Integer) root_gb = Column(Integer) ephemeral_gb = Column(Integer) flavorid = Column(String(255)) swap = Column(Integer, nullable=False, default=0) rxtx_factor = Column(Float, nullable=False, default=1) vcpu_weight = Column(Integer, nullable=True) disabled = Column(Boolean, default=False) is_public = Column(Boolean, default=True)","link":"/2014/05/16/using-sqlalchemy-crud-openstack-nova-flavors/"},{"title":"Linux Iscsi Server Client Settings","text":"#服务器端 ###创建ISCSI设备 tgtadm –lld iscsi –op new –mode target –tid 2 -T iqn.cloud.test:storage.disk2 tgtadm –lld iscsi –op new –mode logicalunit –tid 2 –lun 2 -b /dev/sdg #添加设备 tgtadm –lld iscsi –op bind –mode target –tid 2 -I ALL # 赋权 #客户端 iscsiadm -m discoverydb [ -hV ] [ -d debug_level ] [-P printlevel] [ -t type -p ip:port -I ifaceN … [ -Dl ] ] | [ [ -p ip:port -t type] [ -o operation ] [ -n name ] [ -v value ] [ -lD ] ] 客户端查看Iscsi设备 iscsiadm -m discovery [ -hV ] [ -d debug_level ] [-P printlevel] [ -t type -p ip:port -I ifaceN … [ -l ] ] | [ [ -p ip:port ] [ -l | -D ] ] [root@devstack:keystone_admin images]# iscsiadm -m discovery -t sendtargets -p 127.0.0.1 127.0.0.1:3260,1 iqn.2010-10.org.openstack:volume-7c9646ef-e1f1-4da4-b230-c4ec7679bb6a 执行命令 iscsiadm -m node [ -hV ] [ -d debug_level ] [ -P printlevel ] [ -L all,manual,automatic ] [ -U all,manual,automatic ] [ -S ] [ [ -T targetname -p ip:port -I ifaceN ] [ -l | -u | -R | -s] ] [ [ -o operation ] [ -n name ] [ -v value ] ] ####查看设备 [root@devstack nova]# iscsiadm -m node -T iqn.2010-10.org.openstack:volume-7c9646ef-e1f1-4da4-b230-c4ec7679bb6a --login Logging in to [iface: default, target: iqn.2010-10.org.openstack:volume-7c9646ef-e1f1-4da4-b230-c4ec7679bb6a, portal: 192.168.0.28,3260] (multiple)Logging in to [iface: default, target: iqn.2010-10.org.openstack:volume-7c9646ef-e1f1-4da4-b230-c4ec7679bb6a, portal: 127.0.0.1,3260] (multiple)Login to [iface: default, target: iqn.2010-10.org.openstack:volume-7c9646ef-e1f1-4da4-b230-c4ec7679bb6a, portal: 192.168.0.28,3260] successful.Login to [iface: default, target: iqn.2010-10.org.openstack:volume-7c9646ef-e1f1-4da4-b230-c4ec7679bb6a, portal: 127.0.0.1,3260] successful. fdisk -l iscsiadm -m session [ -hV ] [ -d debug_level ] [ -P printlevel] [ -r sessionid | sysfsdir [ -R | -u | -s ] [ -o operation ] [ -n name ] [ -v value ] ] iscsiadm -m iface [ -hV ] [ -d debug_level ] [ -P printlevel ] [ -I ifacename | -H hostno|MAC ] [ [ -o operation ] [ -n name ] [ -v value ] ] [ -C ping [ -a ip ] [ -b packetsize ] [ -c count ] [ -i interval ] ] iscsiadm -m fw [ -l ] iscsiadm -m host [ -P printlevel ] [ -H hostno|MAC ] [ [ -C chap [ -o operation ] [ -v chap_tbl_idx ] ] | [ -C flashnode [ -o operation ] [ -A portal_type ] [ -x flashnode_idx ] [ -n name ] [ -v value ] ] ]iscsiadm -k priority #使用中遇到的问题 ###iscsiadm: Could not perform SendTargets discovery iscsiadm -m discovery -t sendtargets -p 192.168.127.2 iscsiadm: Could not scan /sys/class/iscsi_transport. iscsiadm: Could not scan /sys/class/iscsi_transport. iscsiadm: can not connect to iSCSI daemon (111)! iscsiadm: Cannot perform discovery. Initiatorname required. iscsiadm: Discovery process to 192.168.127.2:3260 failed to create a discovery session. iscsiadm: Could not perform SendTargets discovery. 是因为没有安装iCSCI daemon ubuntu下安装 open-iscsi（apt-get install open-iscsi） Linux（Centos 6.4）下安装iscsi-initiator-utils-6.2.0.873-10.el6.x86_64","link":"/2014/05/25/linux-iscsi-server-client-settings/"},{"title":"Openstack Havana Neutron 虚拟网络设备分析","text":"Openstack网络设计中有：Tap设备、veth对，linux 桥接、OvS 桥接四中虚拟网络设备。对于一个流经vm中的eth0到物理host的eth1的以太网数据帧来说，要利用host上的9个设备完成：Tap设备vnet0(vm nic),linux 桥接qbrXXX, veth pair(qvbXXX,qvoXXX),Open vSwitch 桥接br-int, veth pair(intbr-eth1,phy-br-eth1),以及最后的物理主机的网卡eth1。 Tap设备:例如KVM、Xen虚拟一个网卡（通常称作VIF或者vNIC）vnet0,供vm使用。Guest OS因此接收到所有发送到Tap设备的以太网数据帧。 Veth pairs 是一对直接相连的虚拟网卡（virtual network interfaces），发送到veth对中的任意一方的以太网数据帧，另一方也会接收到。网络因此利用veth pairs作为VPC(virtual patch cables)来连接virtual bridges. Linux brige 象一个hub一样，可以将多个网络设备，包括物理或者虚拟机的，连接到一个Linux 桥接设备上。以太网数据帧会在hub上相连的的所有网络设备上进行传输 Open vSwitch 桥接设备的想一个虚拟的交换机，网卡连接到OVS 桥街上的端口，端口可以象物理交换机一样，对其进行配置VLAN等信息 由于openstack现在的防火墙的实现机制，使得不能够直接将Tap设备（vnet0、vnet1……）直接接入到桥接设备br-int(like a hub).防火墙实现机制是在tap设备上通过iptables实现防火墙，但是OVS又不支持直接连接到OVS port上的tap设备上的iptables 因此，网络设计上，增加了一个Linux 桥接设备以及一个veth pair来解决这个问题。将tap设备vnet0连接到linux bridge上（qbrXXX），而不是直接到连到ovs的桥街上，这个ovs桥接设备通过（qvbXXX,qvoXXX）veth pair连接到br-int(linke a hub)上 [root@devstack:keystone_admin ~]# brctl show bridge name bridge id STP enabled interfaces qbr69e73747-c0 8000.52a89a03f5a9 no qvb69e73747-c0 qbrff6ee26c-fc 8000.febb4e384e96 no qvbff6ee26c-fc","link":"/2014/05/19/openstack-havana-virtual-networking-devices/"},{"title":"Git Revert Merge","text":"##Git Merge命令的使用及撤销 Git 的 revert 命令可以用来撤销提交（commit），对于常规的提交来说，revert 命令十分直观易用，相当于做一次被 revert 的提交的「反操作」并形成一个新的 commit，但是当你需要撤销一个合并（merge）的时候，事情就变得稍微复杂了一些。 ###Merge Commit在描述 merge commit 之前，先来简短地描述一下常规的 commit。每当你做了一批操作（增加、修改、或删除）之后，你执行 git commit 便会得到一个常规的 Commit。执行 git show &lt;commit&gt;将会输出详细的增删情况。 但是Merge commit 不是这样。每当你使用 git merge 合并两个分支，你将会得到一个新的 merge commit。执行 git show 之后，会有类似的输出： 1 commit 19b7d40d2ebefb4236a8ab630f89e4afca6e9dbe 2 Merge: b0ef24a cca45f9 3 ...... 其中，Merge 这一行代表的是这个合并 parents，它可以用来表明 merge 操作的线索。 举个例子，通常，我们的稳定代码都在 master 分支，而开发过程使用 dev 分支，当开发完成后，再把 dev 分支 merge 进 master 分支： a -> b -> c -> f -- g -> h (master) \\ / d -> e (dev) 上图中，g 是 merge commit，其他的都是常规 commit。g 的两个 parent 分别是 f 和 e。 ###Revert a Merge Commit(撤销merge操作)当你使用 git revert 撤销一个 merge commit 时，如果除了 commit 号而不加任何其他参数，git 将会提示错误： 1 $ git revert g 2 error: Commit g is a merge but no -m option was given. 3 fatal: revert failed 这是由于，在你合并两个分支并试图撤销时，Git 并不知道你到底需要保留哪一个分支上所做的修改。从 Git 的角度来看，master 分支和 dev 在地位上是完全平等的，只是在 workflow 中，master 被人为约定成了「主分支」。 于是是 Git 需要你通过 m 或 mainline 参数来指定「主线」。merge commit 的 parents 一定是在两个不同的线索上，因此可以通过 parent 来表示「主线」。m 参数的值可以是 1 或者 2，对应着 parent 在 merge commit 信息中的顺序。 以上面那张图为例，我们查看 commit g 的内容： $ git show g commit g Merge: f e 那么$ git revert -m 1 g 将会保留 master 分支上的修改，撤销 dev 分支上的修改。 撤销成功之后，Git 将会生成一个新的 Commit，提交历史就成了这样： a -> b -> c -> f -- g -> h -> G (master) \\ / d -> e (dev) 其中 G 是撤销 g 生成的 commit。通过 $ git show G 之后，我们会发现 G 是一个常规提交，内容就是撤销 merge 时被丢弃的那条线索的所有 commit 的「反操作」的合集。 上面的提交历史在实践中通常对应着这样的情况： 工程师在 master 分支切出了 dev 分支编写新功能，开发完成后合并 dev 分支到 master 分支并上线。上线之后，发现了 dev 分支引入了严重的 bug，而其他人已经在最新的 master 上切出了新的分支并进行开发，所以不能简单地在 master 分支上通过重置（git reset ）来回滚代码，只能选择 revert 那个 merge commit。 但是事情还没有结束。工程师必须切回 dev 分支修复那些 bug，于是提交记录变成了这个样子： a -> b -> c -> f -- g -> h -> G -> i (master) \\ / d -> e -> j -> k (dev) 工程师返回 dev 分支通过 j，k 两个 commit 修复了 bug，其他工程师在 master 上有了新的提交 i。现在到了 dev 分支的内容重新上线的时候了。 直觉上来说，还是和之前一样，把 dev 分支合并到 master 分支就好了。于是： $ git checkout master $ git merge dev 得到的提交记录变成了这样： a -> b -> c -> f -- g -> h -> G -> i -- m (master) \\ / / d -> e -> j -> k --------- (dev) m是新的 merge commit。需要注意的是，这不能得到我们期望的结果。因为 d 和 e 两个提交曾经被丢弃过，如此合并到 master 的代码，并不会重新包含 d 和e 两个提交的内容，相当于只有 dev 上的新 commit 被合并了进来，而 dev 分支之前的内容，依然是被 revert 掉了。 所以，如果想恢复整个 dev 所做的修改，应该： $ git checkout master $ git revert G $ git merge dev 于是，提交历史变成了这样： a -> b -> c -> f -- g -> h -> G -> i -> G' -- m (master) \\ / / d -> e -> j -> k --------------- (dev) 其中 G&#39; 是这次 revert 操作生成的 commit，把之前撤销合并时丢弃的代码恢复了回来，然后再 merge dev 分支，把解 bug 写的新代码合并到 master 分支。 现在，工程师可以放心地上线没有 bug 的新功能了。","link":"/2014/05/30/git-revert-merge/"},{"title":"Cloudstack Cli Cloudmonkey Installation","text":"##Cli安装使用分为四部分 安装virtualenv 安装cloudmonkey 配置cloudmonkey 命令使用 安装virtualenv1. virtualenv提供了一个安装软件时的测试环境，所安装的软件不会安装在系统目录下 2. 安装python2.7.4 时的依赖软件， 此步骤可选， cloudmonkey 要求python版本在2.5以上即可 3. yum -y groupinstall &quot;Development tools&quot; zlib-devel bzip2-devel openssl-devel ncurses-devel mysql-devel ibxml2-devel libxslt-devel unixODBC-devel sqlite sqlite-devel源码安装python2.7.4wget http://www.python.org/ftp/python/2.7.4/Python-2.7.4.tar.bz2 tar xf Python-2.7.4.tar.bz2 cd Python-2.7.4 /configure --prefix=/opt/python2.7 make && make altinstall` ##源码安装virtualenv wget https://pypi.python.org/packages/source/v/virtualenv/virtualenv-1.9.1.tar.gz#md5=07e09df0adfca0b2d487e39a4bf2270a tar -xvzf virtualenv-1.9.1.tar.gz python virtualenv -1.9.1/setup.py install ###安装cloudmonkey 建立virtualenv环境 #mkidr workenv #virtualenv worksenv #source worksenv/bin/activate 安装cloudmonkey两种方式任选其一 2.1. 通过python库安装 在系统安装了pip后或者env生效后执行下面命令 #$ pip install cloudmonkey 2.2. 通过源码安装将打包后的源码上传至服务器中解压缩打包后的文件安装 # python setup.py build # # python setup.py install 配置cloudmonkey #cloudmonkey > set history_file /usr/share/cloudmonkey_history > set log_file /var/log/cloudmonkey 设置管理服务器 > set host 192.168.56.1 设置端口 > set port 8080 设置使用的apikey > set apikey {put-your-api-key-for-your-user} 设置使用的secretkey > set secretkey {put-your-secret-key-for-your-user} 修改cloudmonkey终端提示符 > set prompt smcloudcli> 同步配置 > sync 324 APIs discovered and cached 配置结束。通常情况下，所有配置项在 ~/.cloudmonkey/config: 均可见 命令使用tab健可以命令补全 #cloudmonkey 加载用户的key smcloudcli>sync smcloudcli>list 设置数据显示方式为table smcloudcli>set display table 设置数据显示方式为json smcloudcli>set display json smcloudcli>list users","link":"/2014/06/01/cloudstack-cli-cloudmonkey-installation/"},{"title":"How To Change Xenserver Default Installtion Partitions","text":"修改root磁盘大小开机选择F2高级模式，输入shell， vi /opt/xensource/installer/constants.py DOM0_MEM=752为DOM0_MEM=2940, 修改root_size=4096为 root_size=10240 输入EXIT修改内存XenServerDomain0默认使用752MB内存，由于每启动一台虚拟机，Domain0中就会启动一个Qemu-DM的进程，占用大约6M的内存空间， 因此在虚拟机数量较多的情况下，我们需要增大Domain0内存以便支持更多的虚拟机运行。 由于Domain0是32位操作系统，故支持的最大内存量为4GB。 更改Domain0内存的方法参考CTX124806-XenServerSingleServerScalabilitywithXenDesktop提到的例子， 更改/boot/exlinux.conf下包含dom0_mem=2940M的Xen命令行。 为了修改先前的设置，完成下面的步骤： 通过XenCenter的控制口或者SSH方式以root身份登录到Domain0。 确保备份一份原始的/boot/extlinux.conf文件。以免未来的修改导致XenServer不能启动。 用vi打开/boot/extlinux.conf。 下面的改动仅添加到labelxe和labelxe-serial部分。 ###进行下面的改动：labelxe#XenServerkernelmboot.c32append/boot/xen.gzdom0_mem=752Mlowmem_emergency_pool=1Mcrashkernel=64M@32Mconsole=com1vga=mode-0x0311—/boot/vmlinuz-2.6-xenroot=LABEL=root-ecpmuteuroxencons=hvcconsole=hvc0console=tty0quietvga=785splash—/boot/initrd-2.6-xen.img ###改变为:labelxe#XenServerkernelmboot.c32append/boot/xen.gzdom0_mem=2940Mlowmem_emergency_pool=1Mcrashkernel=64M@32Mconsole=com1vga=mode-0x0311—/boot/vmlinuz-2.6-xenroot=LABEL=root-ecpmuteuroxencons=hvcconsole=hvc0console=tty0quietvga=785splash—/boot/initrd-2.6-xen.img 调整Dom0内存设置dom0_mem=2940M是为了分配给dom0更多的内存，这意味着它可以更好地处理更多数量的虚拟机。在改变了这个设置并重启以确保新配置的dom0内存大小生效","link":"/2014/12/01/how-to-change-xenserver-default-installtion-partitions/"},{"title":"Instance start failed because mismatch in VR ssh key Pair","text":"ExceptionFailed to authentication SSH user root on host 10.147.40.1642013-06-15 01:36:13,977 ERROR [vmware.resource.VmwareResource] (DirectAgent-18:10.147.40.30) Unable to execute NetworkUsage command on DomR (10.147.40.164), domR may not be ready yet. failure due to Exception: java.lang.ExceptionMessage: Failed to authentication SSH user root on host 10.147.40.164java.lang.Exception: Failed to authentication SSH user root on host 10.147.40.164at com.cloud.utils.ssh.SshHelper.sshExecute(SshHelper.java:144)at com.cloud.utils.ssh.SshHelper.sshExecute(SshHelper.java:37)at com.cloud.hypervisor.vmware.resource.VmwareResource.networkUsage(VmwareResource.java:5451)at com.cloud.hypervisor.vmware.resource.VmwareResource.execute(VmwareResource.java:2301)at com.cloud.hypervisor.vmware.resource.VmwareResource.executeRequest(VmwareResource.java:480)at com.cloud.agent.manager.DirectAgentAttache$Task.run(DirectAgentAttache.java:186)at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)at java.util.concurrent.FutureTask.run(FutureTask.java:166)at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:165)at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:266)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)at java.lang.Thread.run(Thread.java:679)2013-06-15 01:36:13,980 DEBUG [vmware.resource.VmwareResource] (DirectAgent-18:10.147.40.30) Executing resource GetDomRVersionCmd: {“accessDetails”:{“router.ip”:”10.147.40.164”,”router.name”:”r-8-VM”},”wait”:0}2013-06-15 01:36:13,980 DEBUG [vmware.resource.VmwareResource] (DirectAgent-18:10.147.40.30) Run command on domR 10.147.40.164, /opt/cloud/bin/get_template_version.sh2013-06-15 01:36:13,981 DEBUG [vmware.resource.VmwareResource] (DirectAgent-18:10.147.40.30) Use router’s private IP for SSH control. IP : 10.147.40.1642013-06-15 01:36:14,081 DEBUG [cloud.api.ApiServlet] (catalina-exec-12:null) ===START=== 10.101.255.119 – GET command=queryAsyncJobResult&amp;jobId=43dde7f6-fb5d-4ead-8691-5071feb44dfd&amp;response=json&amp;sessionkey=tN07%2BJ4GVSCHGNV%2FPjdO3V%2Bs3Tg%3D&amp;=13712208480202013-06-15 01:36:14,163 DEBUG [cloud.api.ApiServlet] (catalina-exec-12:null) ===END=== 10.101.255.119 – GET command=queryAsyncJobResult&amp;jobId=43dde7f6-fb5d-4ead-8691-5071feb44dfd&amp;response=json&amp;sessionkey=tN07%2BJ4GVSCHGNV%2FPjdO3V%2Bs3Tg%3D&amp;=13712208480202013-06-15 01:36:14,231 ERROR [utils.ssh.SshHelper] (DirectAgent-18:10.147.40.30) Failed to authentication SSH user root on host 10.147.40.1642013-06-15 01:36:14,235 ERROR [vmware.resource.VmwareResource] (DirectAgent-18:10.147.40.30) GetDomRVersionCmd failed due to Exception: java.lang.ExceptionMessage: Failed to authentication SSH user root on host 10.147.40.164java.lang.Exception: Failed to authentication SSH user root on host 10.147.40.164at com.cloud.utils.ssh.SshHelper.sshExecute(SshHelper.java:144)at com.cloud.utils.ssh.SshHelper.sshExecute(SshHelper.java:37)at com.cloud.hypervisor.vmware.resource.VmwareResource.execute(VmwareResource.java:2143)at com.cloud.hypervisor.vmware.resource.VmwareResource.executeRequest(VmwareResource.java:488)at com.cloud.agent.manager.DirectAgentAttache$Task.run(DirectAgentAttache.java:186)at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)at java.util.concurrent.FutureTask.run(FutureTask.java:166)at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:165)at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:266)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)at java.lang.Thread.run(Thread.java:679)2013-06-15 01:36:14,237 DEBUG [agent.manager.DirectAgentAttache] (DirectAgent-18:null) Seq 1-965476380: Cancelling because one of the answers is false and it is stop on error.2013-06-15 01:36:14,238 DEBUG [agent.manager.DirectAgentAttache] (DirectAgent-18:null) Seq 1-965476380: Response Received ##Solutions [root@ms2 ~]# cat /var/cloudstack/management/.ssh/id_rsa.pubssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAuqhNj72tdEVhc/Cbph6EyPLcPm6KKrgVMdFKXRn7uW5S6UT+QqtM4ec7S0lA0roVsbnst2/DdLQZhCcBDMggRRP7BJSM1O5cXcFQib+gwAaneaUwomVMPiA/uAdCJBxgId5qTZw4cyWnemLkkZTcta96aIIhT0/ycal4PMPbXRE9u78RRVeoo19BnjAYd6o7zo+O8fNaRLdQRTcpGCm5KTQRhKREOA/OREHvX0mBuuEAie74e3QVd+ZA/6kQ7uspKsFnQTWEM0I8YR2UPj9JL1pyPAawV3QiycEOaChYhUZ6DcuebEJEbzahiRbtBnDkRz+gQ/TpHx96pmAzzDaQyQ==cloud@ms2[root@ms2 ~]####Mount System ISO[root@ms2 ~]# mkdir /tmp/is[root@ms2 ~]# mount -o loop /var/cloudstack/mnt/VM/7566222426160.3051fff0/systemvm/systemvm-4.2.0-SNAPSHOT.iso root@ms2 ~]# cd /tmp/is/ [root@ms2 is]# lsauthorized_keys cloud-scripts.tgz systemvm.zip [root@ms2 is]# cat authorized_keysssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAv2Ym6oJKNnSiMPg0F/RxNhCNvmiB0HJwWLbWC0GQLkB6/Q7F93X8jConFbjehQqBPXePZhEV8veGyH8AmjRDlnP42qpxvCsOnD7BGrXm+Uv+/VovzZ719wcMny0GO2spJgpJcut0YrJqr9OXRlrgbis9dweeB0SyZDKT2ja4c6V2E2Fo9D7BdqWg2S2ptKW7oNkL6846Rfa9/WHh0PO7DmJ5mIuJbazNfBOM0SH6M2FOORgBFytC2XEf0o2aYHGDKOyuwDb2YOuThfDNP3r29qhRUHdZY8Ozs9S0iF+m/L3vBvuOCk0eJMjuYJnsuMTH7KScP5GILurlBCaU9vXmvw== cloud@ms2 We can see that the private keys are not equal, so we update the system.iso with correct authorized_keys, and then replace system.iso, destroy ssvm ,cpvm along with it’s base template image within hypervisor, then the new system vms will be bring up. Now you can deploy you virtual machines","link":"/2014/09/03/instance-start-failed-because-mis-match-in-vr-ssh-key-pair/"},{"title":"How To Publish Events Into AMPQ/Rabbitmq Within Cloudstack 4.4.1","text":"Currently within cloudstack all events are stored in DB and the only to retrive events is using APIS which is not so convient.Since 4.1.0, Apache CloudStack began to support publishing interesting events from the management servers onto a message queue. The current implementation of that feature uses RabbitMQ as the message broker.Fowllowings are the steps to enable such feature. Install Rabbitmq-server Following the instructions from here:http://www.rabbitmq.com/download.html to setup rabbitmq-server Make sure that rabbitmq-server running, and (/or) enable rabbitmq-management webui plugin Configure CloudstackAssuming you have setup cloudstack management server and have mq server running with correct user credential(Default user is guest/guest) In cloudstack management server Create spring event bus related configuration file mdir -p /usr/share/cloudstack-management/webapps/client/WEB-INF/classes/META-INF/cloudstack/core [root@cs-mgmt core]# cat spring-event-bus-context.xml < beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:aop=\"http://www.springframework.org/schema/aop\" xmlns:util=\"http://www.springframework.org/schema/util\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-3.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util-3.0.xsd\" > < bean id=\"eventNotificationBus\" class=\"org.apache.cloudstack.mom.rabbitmq.RabbitMQEventBus\"> < property name=\"name\" value=\"eventNotificationBus\" /> < property name=\"server\" value=\"localhost\" /> < property name=\"port\" value=\"5672\" /> < property name=\"username\" value=\"guest\" /> < property name=\"password\" value=\"guest\" /> < property name=\"exchange\" value=\"cloudstack-events\" /> < /bean> < /beans> restart cloudstack management service /etc/init.d/cloudstack-management restart ###Check Exchange status Using rabbitmqctl to check the status exchanges of current node. [root@cs-mgmt core]# rabbitmqctl list_exchanges Listing exchanges ... direct amq.direct direct amq.fanout fanout amq.headers headers amq.match headers amq.rabbitmq.log topic amq.rabbitmq.trace topic amq.topic topic cloudstack-events topic ...done. [root@cs-mgmt core]# We can see that cloudstack has connected to rabbitmq server.","link":"/2014/12/17/how-to-publish-events-into-ampqrabbitmq-within-cloudstack-441/"},{"title":"Enable screen tabs name ","text":"This introduction will outline specifically on how to show tab name on an active screen Linux screen is a very powerful tool for sysadmin when you need to run a long running process without kept your computer connecting to the remote server here is the deatils introductions about Screen There is two ways to display tab name Configure .screenrc Add this to your .screenrc file: `caption always &quot;%{= kw}%-w%{= BW}%n %t%{-}%+w %-= @%H - %LD %d %LM - %c&quot;` After you restart your screen, there&apos;s a status bar below showing the current tab name, and as a bonus your current host name and time -- modify them away at will if you so wish. To rename a tab, press ctrl+a A and give it a new name. Operate at existing screen Sometimes you create a screen but havn’t enable captions, it’s not convient to switch among tabs, you can run following commands to enable screen tabs First you need to detach current screen ctrl + a +d screen -S &lt;YOUR SCREEN NAME HERE&gt; -X caption always &quot;%{= kw}%-w%{= BW}%n %t%{-}%+w %-= @%H - %LD %d %LM - %c&quot; Attach to your screen screen -r &lt;YOUR SCREEN NAME HERE&gt; Now your screen will be looks like this","link":"/2015/07/21/howto-enable-linux-screen-tabs-name/"},{"title":"PowerDNS Configuration For Single Node","text":"Configuration For Single HostThe guide is used for setup a single node powerdns with mysql backend under openstack Installation Launch a powerdns instance using image ubuntu 14.04, associate floatingip ssh into instance then run following commands sudo apt-get update sudo apt-get install mysql-server mysql-common pdns-server pdns-backend-mysql WHEN INSTALLING MYSQL-SERVER, REMEMBER THE ROOT PASSWORD OF DATABASE FOR LATER USE Configuration configure mysql-server edit /etc/mysql/my.cnf bind-address = 127.0.0.1 restart mysql-server Create powerdns database Now that you have created a database, you need to add a user that will have access to that database. For simplify we are using root user, otherwise create use and grant access to database. Next, you create the database required for your install of PowerDNS: create database pdns; use pdns; create table domains ( id INT auto_increment, name VARCHAR(255) NOT NULL, master VARCHAR(128) DEFAULT NULL, last_check INT DEFAULT NULL, type VARCHAR(6) NOT NULL, notified_serial INT DEFAULT NULL, account VARCHAR(40) DEFAULT NULL, primary key (id) ) Engine=InnoDB; CREATE UNIQUE INDEX name_index ON domains(name); CREATE TABLE records ( id INT auto_increment, domain_id INT DEFAULT NULL, name VARCHAR(255) DEFAULT NULL, type VARCHAR(10) DEFAULT NULL, content VARCHAR(64000) DEFAULT NULL, ttl INT DEFAULT NULL, prio INT DEFAULT NULL, change_date INT DEFAULT NULL, primary key(id) ) Engine=InnoDB; CREATE INDEX nametype_index ON records(name,type); CREATE INDEX domain_id ON records(domain_id); create table supermasters ( ip VARCHAR(64) NOT NULL, nameserver VARCHAR(255) NOT NULL, account VARCHAR(40) DEFAULT NULL ) Engine=InnoDB; Now, leave the MySQL shell with: quit; modify powerdns server cd /etc/powerdns mv pdns.conf pdns.conf.bak cp pdns.d/pdns.local.gmysql.conf pdns.conf then modify pdns.conf change to the db credentials created above. Finally, restart your PowerDNS service with the following: sudo service pdns restart Admin Web UI Poweradmin writen in php give you a web dashboard for operating dns record, much more convinent. Install the prerequisites $ sudo apt-get install apache2 libapache2-mod-php5 php5 php5-common php5-curl php5-dev php5-gd php-pear php5-imap php5-mcrypt php5-common php5-ming php5-mysql php5-xmlrpc gettext $sudo pear install MDB2 $sudo pear install MDB2_Driver_mysql Download poweradmin release $cd /tmp wget https://github.com/downloads/Poweradmin/Poweradmin/Poweradmin-2.1.6.tgz tar xvfz Poweradmin-2.1.6.tgz mv Poweradmin-2.1.6 /var/www/html/Poweradmin touch /var/www/html/Poweradmin/inc/config.inc.php chown -R www-data:www-data /var/www/html/Poweradmin/` Now open a browser and launch the web-based Poweradmin installer (http:///poweradmin/install). Following the guide to setup admin ui. After installation, install subfolder under /var/www/html/Poweradmin need to be removed or migrated, then go to http:///Poweradmin//index.php to setup dns records FAQ Can not launch powerdns admin installer Login to the powerdns machine, disable firewall of add the tcp:80 port to the INPUT chain, restart apache2 service Mysql crypto releate issue Make sure php5-mcrypt installed(yum install php5-mcrypt) modify configuration for php root@powerdns-cli-01:/etc/php5# find / -name mcrypt.so /usr/lib/php5/20121212/mcrypt.so root@powerdns-cli-01:/etc/php5# cat cli/conf.d/20-mcrypt.ini ; configuration for php MCrypt module extension=/usr/lib/php5/20121212/mcrypt.so` References https://doc.powerdns.com/md/authoritative/installation/ Google","link":"/2015/05/06/powerdns-configuration-for-single-node/"},{"title":"How to enable custom-domain and readmore in Hexo","text":"What’s is Hexo ? according to the official introduction its a blog framework, have more than 20k forks and large user base. “A fast, simple &amp; powerful blog framework” The deployment/usage is quite strightforward but not that easy to find ways to manage the custom domain in github pages and the “Read More” in minos theme The website you are visting is built on top Hexo and Minos theme and hosting in github pages with a CNAME for dns resolvtion How we made it Generate the CNAME file in source directory1echo \"ACTUAL Website DOMAIN\" &gt; source/CNAME if the site is deployed to github before without the CNAME file, need to delete public folder otherwise the change won’t take into effectthen update your dns server make it point to the desied github site, for me its 12345678910111213141516171819$ dig www.ystacks.com; &lt;&lt;&gt;&gt; DiG 9.11.3-1ubuntu1.8-Ubuntu &lt;&lt;&gt;&gt; www.ystacks.com;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 42551;; flags: qr rd ra; QUERY: 1, ANSWER: 5, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;www.ystacks.com. IN A;; ANSWER SECTION:www.ystacks.com. 1498 IN CNAME jiangytcn.github.io.jiangytcn.github.io. 1498 IN A 185.199.109.153jiangytcn.github.io. 1498 IN A 185.199.110.153jiangytcn.github.io. 1498 IN A 185.199.108.153jiangytcn.github.io. 1498 IN A 185.199.111.153 Enable the ReadMore for the posts add &lt;!---more ---&gt; section in the post articles, and whatever above it would be present as the abstract in the list page like below That’s it.","link":"/2019/08/10/How-to-enable-custom-domain-and-readmore-in-Hexo/"},{"title":"KVM guest and host cpu binding","text":"kvm虚拟出来的虚拟机（vm）是运行在单独的一个逻辑cpu还是可以分别在各个cpu之间运行？虚拟机cpu（vcpu）是什么概念？物理机（host）怎么看待kvm和vcpu? 为了搞懂这个概念我们还是要回到命令行中看。举例说明：我这里有一个虚拟机叫core8，它含有8个虚拟cpu它的进程编号是20736。 就是说core8在host看来就是一个进程而已，这个集成的编号是20736.那么现在提出一个问题，这个core8的8个vcpu是怎么个情况呢？在哪里运行呢？这时还是得借助命令行。我们在host里使用ps指令，但是不能单纯用ps，还要借助于参数： ps -eL （e的意思是打印所有进程，L的意思是连县城也不放过）。我这里只显示一下和我们的20736进程相关的信息： kvm虚拟机vcpu资源绑定_第1张图片 你会看到和20736相关的有九行，那么这九行是什么呢？首先第一列都是20736，第二列里只有第一行是20736，后面的都不是。那么我们这时就应该明白了，对于host来说，kvm虚拟机是一个进程（20736），虚拟机的vcpu都是这个进程衍生出来的线程。这就是为什么除了20736还有另外八行的原因。 那么我们接着询问，这八个线程是跑在同一个逻辑cpu里吗？为了回答这个问题，我们接着做实验： 还是借助于ps指令： ps -eLo ruser,pid,ppid,lwp,psr| awk ‘{if($5==1) print $0}’解释为：ps命令显示当前系统的进程信息的状态，它的“-e”参数用于显示所有的进程，“-L”参 数用于将线程（LWP，light-weight process）也显示出来，“-o”参数表示以用户自定义的格式输出（其中“psr”这列表示当前分配给进程运行的处理器编号，“lwp”列表示线程的 ID，“ruser”表示运行进程的用户，“pid”表示进程的ID，“ppid”表示父进程的ID，）。结合ps和 awk工具的使用，是为了分别打印出来运行在不同的逻辑cpu上的进程线程情况。上面的指令就是打印出1号（从0开始编号）cpu的进行线程情况，我们这里只列出和我们相关的： 这时看到，20736号进程衍生出来的线程只有一部分运行在逻辑cpu1上，其它的线程在其它的cpu上了。 这时就大概明白了，不同的vcpu只是不同的线程，而不同的线程是跑在不同的cpu上的。 Qemu/kvm为客户机提供一套完整的硬件系统环境，在客户机看来其所拥有的cpu即是vcpu（virtual CPU）。在KVM环境中，每个客户机都是一个标准的Linux进程（qemu进程），而每一个vCPU在宿主机中是Qemu进程派生的一个普通线程。 KVM中的一个客户机作为一个用户空间进程（qemu-kvm）运行的，它和其他普通的用户进程一样由内核来调度使其运行在物理cpu上，不过它由KVM模块控制。多个客户机就是宿主机中的多个QEMU进程，而一个客户机的多个vCPU就是一个QEMU进程中的多个线程。在客户机系统中，同样分别运行着客户机的内核和客户机的用户空间应用程序。疑问： 列表 KVM环境下的smp的架构是如何处理时限的，也就是cores * 2，socket * 2 ，threads * 2与 cores * 1，socket * 8，thread * 1 有什么本质的区别？ 因为为虚拟机提供计算的vcpu在宿主机上实际为一个线程，线程上并不会对cores、sockets或者threads做明确区分，也就是整体设置222 和 181是没区别的。不过在虚拟机内部，可能会有系统倾向于（服务器的设别程度）多物理插槽，或者多物理核心。（之前遇到的win2k8（非Datacenter）的情况就是，无法识别超过8个的vcpu，但是设置成261 即可实现12个vcpu的服务器 进程的处理器亲和性和VCPU绑定什么是进程亲和性：简单的一句话就是我这个Linux进程到底可以在哪几个cpu上做负载均衡。 KVM虚拟机是一个普通的linux进程，vcpu是一个线程，我们可以在宿主机上将vcpu线程对应的tid绑定到指定的cpu上。 应用场景用户希望把虚拟机的VCPU绑定在特定物理CPU上，VCPU只在绑定的物理CPU上调度，达到隔离VCPU并提升虚拟机性能的目的。如果没有作VCPU绑定，则虚拟机的VCPU可以在所有物理CPU上调度。 绑定vcpu到指定的cpu上，的确会提高性能，因为在vcpu的线程在物理cpu做负载均衡的时候，会有一些必要的数据结构初始化（vmlaunch）相对于VM-Entry来说是比较奢侈的，加上cache的命中，性能必然会有所提高，但破坏了负载均衡。当绑定在同一cpu上的两个vcpu同时高负载的时候，性能就会大打折扣，而其他的cpu也没有得到充分的利用。 在KVM环境中，一般并不推荐手动设置qemu进程的处理器亲和性来绑定vCPU，但是，在非常了解系统硬件架构的基础上，根据实际应用的需求，可以将其绑定到特定的CPU上，从而提高客户机中的CPU执行效率或实现CPU资源独享的隔离性。 为了实现这个功能，你首先得会taskset命令，直观上来说，taskset就是设置任务，也就是制定任务运行的情况，是一个很好用的工具。 1 taskset -p [mask] pid taskset绑定进程到某个CPU是很方便的：#taskset -pc 0,1 1249这会绑定1249进程到0号跟1号cpu上。 #cat /proc/1249/statusCpus_allowed: 3Cpus_allowed_list: 0-1 重新绑定下： #taskset -pc 1 1249#cat /proc/1249/statusCpus_allowed: 2Cpus_allowed_list: 1 注意这里的Cpu_allowed用的是 二进制掩码，3的二进制是11，2的二进制是10。前一个表示可在两个CPU上运行，第二个表示仅在第二个CPU上运行。那么我们这里就可以使用taskset了，只需把这九个线程都绑定在同一个cpu上即可。假设我们把这个虚拟机绑定到1号cpu上：taskset -p 2 20736taskset -p 20740taskset -p 20741……taskset -p 20747 ok,这时你再运行ps -eLo ruser,pid,ppid,lwp,psr| awk ‘{if($5==1) print $0}’，会看到形如下面的结果： 那么也就是完成了我们的虚拟机绑定任务，为了验证一下是否真正实现了绑定，我们在core8虚拟机里运行一个死循环，然后看host里的 top指令的结果： 我们可以清楚的看到1号cpu的利用率100%，而其它的cpu基本上没用到，这说明我们的绑定是成功的，完成了客户提出的需求。 Taskset命令设置某虚拟机在某个固定cpu上运行 设置某个进程pid在某个cpu上运行： [root@test~]# taskset -p 100 95090pid 95090’s current affinity mask: 1pid 95090’s new affinity mask: 100 解释：设置95090这个进程，在cpu8上运行 95090是我提前用ps –aux|grep “虚拟机名” 找到的虚拟机进程id。 vcpupin命令设置虚拟机某个vcpu在某个固定cpu上运行vcpupin的命令如下：virsh vcpupin 4 0 8：绑定domain 4的vcpu 0 到物理CPU8taskset和vcpupin区别Taskset是以task（也就是虚拟机）为单位，也就是以虚拟机上的所有cpu为一个单位，与物理机上的cpu进行绑定，它不能指定虚拟机上的某个vcpu与物理机上某个物理cpu进行绑定，其粒度较大。vcpupin命令就可以单独把虚拟机上的vcpu与物理机上的物理cpu进行绑定 比如vm1和vm2都有3个vcpu（core），物理机有8个cpu（8个core，假如每个core一个线程），taskset能做到把vm2的3个vcpu同时绑定到一个或者多个cpu上，但vcpupin能把vm1的每个vcpu与每个cpu进行绑定。","link":"/2019/08/10/KVM-guest-and-host-cpu-binding/"},{"title":"KVM guest host hardware passthrough","text":"在vmware下，物理主机的硬件映射给虚拟机称为直通, 可以很方便的通过ＵＩ进行操作, 但kvm下，需要通过技术手段实现 kvm下的DMA（direct memory access） dma要存取的内存地址成为DMA地址（也称为bus address），dmar是DMA remapping ,是intel为支持虚拟机而设计的I/O虚拟化技术，I/O设备访问的DMA地址不再是物理内存地址，而要通过DMA remapping硬件进行转译，DMA remapping硬件会把DMA地址翻译成物理内存地址，并检查访问权限等等。负责DMA remapping操作的硬件称为IOMMU。 两种DMAR的实现方法均需要IOMMU 方法一 pci-stub开启IOMMU，之后modprobemodprobe pci_stub 之后会有/sys/bus/pci/drivers/pci-stub这个模块 1234lspci -nnD 查看设备详细信息echo \"14e4 1639\" &gt; /sys/bus/pci/drivers/pci-stub/new_idecho 0000:01:00.1 &gt; /sys/bus/pci/devices/0000:01:00.1/driver/unbindecho 0000:01:00.1 &gt; /sys/bus/pci/drivers/pci-stub/bind 使用qemu-kvm启动虚拟机 /usr/libexec/qemu-kvm -enable-kvm -m 8192 -smp 4,maxcpus=16,sockets=16,cores=2,threads=2 -spice port=3003,disable-ticketing -vga qxl /home/vms/images/basewin7.img -device pci-assign,host=01:00.1 方法二 vfio开启IOMMU，之后加载内核模块 123456mdoprobe vfiomodprobe vfio-pcilspci -nnDecho 0000:05:00.0 &gt; /sys/bus/pci/devices/0000:05.00.0/driver/unbindecho 1000 005b &gt; /sys/bus/pci/drivers/vfio-pci/new_id 使用qemu-kvm启动虚拟机 /usr/libexec/qemu-kvm -M pc-i440fx-rhel7.0.0 -enable-kvm -m 2048 -smp 2 -drive if=pflash,format=raw,file=/usr/share/edk2.git/ovmf-x64/OVMF_CODE-pure-efi.fd -drive id=disk0,if=none,format=qcow2,file=test.qcow2 -device virtio-blk-pci,drive=disk0,bootindex=0 -global PIIX4_PM.disable_s3=0 -global isa-debugcon.iobase=0x402 -debugcon file:fedora.ovmf.log -monitor stdio -device piix3-usb-uhci -device usb-tablet -netdev id=net0,type=user -device virtio-net-pci,netdev=net0,romfile= -device qxl-vga -spice port=3000,disable-ticketing -device vfio-pci,host=00:05:00.0","link":"/2019/08/10/KVM-guest-host-hardware-passthrough/"},{"title":"OpenStack AIO Setup with Ansible","text":"如果你是OpenStack 的developer的话，那么相信对DevStack的会十分的熟悉。 DevStack 可以在一台机器上快速的交付AIO（all-in-one）的OpenStack环境十分的方便。 OpenStack-Ansible 是一种通过ansible来部署OpenStack 环境的方案，具体的设计初衷、以及如果使用请参考rackspace 以及platform9开发博客 物理环境 名称 信息 OS Dist Ubuntu 16.04.2 LTS Disk 1T + 2T sata 盘 Mem 32G CPU Intel(R) Core(TM) i7-6820HQ CPU @ 2.70GHz 12345678sdb 8:16 0 931.5G 0 disk├─sdb1 8:17 0 243M 0 part├─sdb2 8:18 0 1K 0 part└─sdb5 8:21 0 931.3G 0 part ├─dvlp-root 252:0 0 78G 0 lvm / ├─dvlp-home 252:1 0 262.3G 0 lvm /home └─dvlp-data 252:2 0 553.7G 0 lvmsdc 8:32 0 1.8T 0 disk /openstack 2T盘用于存储OpenStack相关的数据1T盘操作系统相关数据盘 在使用时遇到的问题在此说明 安装时启动的lxc container 无法获得private ip正常情况下在执行了ansible 相关命令后应该每一个container应该有内网及外网IP， 如下1234567891011121314151617181920212223242526→ sudo lxc-ls -fNAME STATE AUTOSTART GROUPS IPV4 IPV6aio1_cinder_api_container-f3bdb579 RUNNING 1 onboot, openstack 10.255.255.244, 172.29.236.67, 172.29.247.153 -aio1_cinder_scheduler_container-25e9e567 RUNNING 1 onboot, openstack 10.255.255.198, 172.29.238.236 -aio1_designate_container-b3ca6e11 RUNNING 1 onboot, openstack 10.255.255.64, 172.29.239.241 -aio1_galera_container-59168dd6 RUNNING 1 onboot, openstack 10.255.255.172, 172.29.236.218 -aio1_glance_container-e43dfd64 RUNNING 1 onboot, openstack 10.255.255.176, 172.29.237.35, 172.29.247.172 -aio1_heat_apis_container-9008349f RUNNING 1 onboot, openstack 10.255.255.112, 172.29.238.189 -aio1_heat_engine_container-2d62ff94 RUNNING 1 onboot, openstack 10.255.255.25, 172.29.239.228 -aio1_horizon_container-2a77938a RUNNING 1 onboot, openstack 10.255.255.74, 172.29.239.99 -aio1_keystone_container-3441527b RUNNING 1 onboot, openstack 10.255.255.28, 172.29.236.169 -aio1_memcached_container-43bf9c7f RUNNING 1 onboot, openstack 10.255.255.238, 172.29.239.33 -aio1_neutron_agents_container-31363c11 RUNNING 1 onboot, openstack 10.255.255.187, 172.29.236.66, 172.29.242.144 -aio1_neutron_server_container-c6d33080 RUNNING 1 onboot, openstack 10.255.255.160, 172.29.236.141 -aio1_nova_api_metadata_container-b04322cb RUNNING 1 onboot, openstack 10.255.255.203, 172.29.237.253 -aio1_nova_api_os_compute_container-53803a56 RUNNING 1 onboot, openstack 10.255.255.61, 172.29.237.166 -aio1_nova_api_placement_container-d09947c4 RUNNING 1 onboot, openstack 10.255.255.22, 172.29.236.157 -aio1_nova_conductor_container-e9e82699 RUNNING 1 onboot, openstack 10.255.255.120, 172.29.239.36 -aio1_nova_console_container-2517dd0e RUNNING 1 onboot, openstack 10.255.255.54, 172.29.239.129 -aio1_nova_scheduler_container-db02b5f9 RUNNING 1 onboot, openstack 10.255.255.243, 172.29.237.239 -aio1_rabbit_mq_container-66fd8455 RUNNING 1 onboot, openstack 10.255.255.2, 172.29.236.124 -aio1_repo_container-cf8b6b77 RUNNING 1 onboot, openstack 10.255.255.17, 172.29.238.136 -aio1_rsyslog_container-d461d2ed RUNNING 1 onboot, openstack 10.255.255.76, 172.29.236.228 -aio1_swift_proxy_container-00c23b79 RUNNING 1 onboot, openstack 10.255.255.20, 172.29.236.166, 172.29.245.74 -aio1_utility_container-9755697d RUNNING 1 onboot, openstack 10.255.255.219, 172.29.239.126 -ubuntu-xenial-amd64 STOPPED 0 - - - 由于之前配置了dnsmasq作为本机的dns server，而lxc 需要用dnsmasq作为所启动的container的dhcp 服务器，所以container 无法通过dhcp获得ip导致deploy失败 解决办法: 清空dnsmasq的配置后，重新deploy container 即可获得内部IP 重启物理节点后无法识别usb设备在OpenStack-Ansible 的使用文档中说明了如何处理在重启物理节点后，openstack 服务如何恢复的问题。在使用时，由于sdc盘通过的usb 接口连接到的server中，重启后出现无法识别存储设备的问题。在ansible 的role 里面有security、security role 相关的设置,在安装好openstack-ansible 后会在host中使用modporb禁用了USB设备 12→ cat /etc/modprobe.d/openstack-ansible-security-disable-usb-storage.confinstall usb-storage /bin/true 解决办法: 删除此文件后，重启物理节点，usb设备即可识别","link":"/2019/08/10/OpenStack-AIO-Setup-with-Ansible/"},{"title":"OpenStack Internal Service Design","text":"API前端服务每个 OpenStack 组件可能包含若干子服务，其中必定有一个 API 服务负责接收客户请求。 以 Nova 为例，nova-api 作为 Nova 组件对外的唯一窗口，向客户暴露 Nova 能够提供的功能。 当客户需要执行虚机相关的操作，能且只能向 nova-api 发送 REST 请求。 这里的客户包括终端用户、命令行和 OpenStack 其他组件。设计 API 前端服务的好处在于： 对外提供统一接口，隐藏实现细节API 提供 REST 标准调用服务，便于与第三方系统集成可以通过运行多个 API 服务实例轻松实现 API 的高可用，比如运行多个 nova-api 进程Scheduler调度服务对于某项操作，如果有多个实体都能够完成任务，那么通常会有一个 scheduler 负责从这些实体中挑选出一个最合适的来执行操作。 Nova 有多个计算节点。当需要创建虚机时，nova-scheduler 会根据计算节点当时的资源使用情况选择一个最合适的计算节点来运行虚机。 Worker工作服务调度服务只管分配任务，真正执行任务的是 Worker 工作服务。 在 Nova 中，这个 Worker 就是 nova-compute 了。 将 Scheduler 和 Worker 从职能上进行划分使得 OpenStack 非常容易扩展： 计算资源不够了无法创建虚机时，可以增加计算节点（增加 Worker）客户的请求量太大调度不过来时，可以增加 SchedulerDriver框架OpenStack 作为开放的 Infrastracture as a Service 云操作系统，支持业界各种优秀的技术。 这些技术可能是开源免费的，也可能是商业收费的。 这种开放的架构使得 OpenStack 能够在技术上保持先进性，具有很强的竞争力，同时又不会造成厂商锁定（Lock-in）。 那 OpenStack 的这种开放性体现在哪里呢？ 一个重要的方面就是采用基于 Driver 的框架。 以 Nova 为例，OpenStack 的计算节点支持多种 Hypervisor。 包括 KVM, Hyper-V, VMWare, Xen, Docker, LXC 等。Nova-compute 为这些 Hypervisor 定义了统一的接口，hypervisor 只需要实现这些接口，就可以 driver 的形式即插即用到 OpenStack 中。 下面是 nova driver 的架构示意图 在 nova-compute 的配置文件 /etc/nova/nova.conf 中由 compute_driver 配置项指定该计算节点使用哪种 Hypervisor 的 driver在我的实验环境中因为是 KVM，所以配置的是 Libvirt 的 driver。 Messaging服务nova-* 子服务之间的调用严重依赖 Messaging。Messaging 是 nova-* 子服务交互的中枢。 程序之间的调用通常分两种：同步调用和异步调用。 同步调用API 直接调用 Scheduler 的接口是同步调用。 其特点是 API 发出请求后需要一直等待，直到 Scheduler 完成对 Compute 的调度，将结果返回给 API 后 API 才能够继续做后面的工作。 异步调用API 通过 Messaging 间接调用 Scheduler 就是异步调用。 其特点是 API 发出请求后不需要等待，直接返回，继续做后面的工作。 Scheduler 从 Messaging 接收到请求后执行调度操作，完成后将结果也通过 Messaging 发送给 API。在 OpenStack 这类分布式系统中，通常采用异步调用的方式，其好处是： 解耦各个子服务子服务不需要知道其他服务在哪里运行，只需要发送消息给 Messaging 就能完成调用。 提高性能异步调用使得调用者无需等待结果返回。这样可以继续执行更多的工作，提高系统总的吞吐量。 提高伸缩性子服务可以根据需要进行扩展，启动更多的实例处理更多的请求，在提高可用性的同时也提高了整个系统的伸缩性。而且这种变化不会影响到其他子服务，也就是说变化对别人是透明的。 DatabaseOpenStack 各组件需要维护自己的状态信息。 比如 Nova 中有虚机的规格、状态，这些信息都是在数据库中维护的。 每个 OpenStack 组件在 MySQL 中有自己的数据库。","link":"/2019/08/10/OpenStack-Internal-Service-Design/"},{"title":"VIM split ticks and tips","text":"VIM is useful and powerful for the generic editor, it has lots of plugins to use for different language and purpose. The vim config file located in $HOME/.vimrc. The configuration is not that easy to make as your plugins increasing and its quite tedious to remember and search “How To Use XXX” for each plugs. But https://vim-bootstrap.com/ provide a handy way to ease the configuration, which saved lots of time. For me, the split feature is useful, which can display multiple file in a singe window, you don’t need to switch from files. The generic tricks for the vim split manipulation 12345678910111213:e filename - edit another file:split filename - split window and load another filectrl-w up arrow - move cursor up a windowctrl-w ctrl-w - move cursor to another window (cycle)ctrl-w_ - maximize current windowctrl-w= - make all equal size10 ctrl-w+ - increase window size by 10 lines:vsplit file - vertical split:sview file - same as split, but readonly:hide - close current window:only - keep only this window open:ls - show current buffers:b 2 - open buffer #2 in this window Once generated the .virmc file by using the vim-bootstrap, the shortcut would be created and you can even create your own shortcut for spcific command. Assuming we opened file and split that into 3 splits Split NavigationsYou can use different key mappings for easy navigation between splits to save a keystroke. So instead of ctrl-w then j, it’s just ctrl-j: 1234nnoremap &lt;C-J&gt; &lt;C-W&gt;&lt;C-J&gt;nnoremap &lt;C-K&gt; &lt;C-W&gt;&lt;C-K&gt;nnoremap &lt;C-L&gt; &lt;C-W&gt;&lt;C-L&gt;nnoremap &lt;C-H&gt; &lt;C-W&gt;&lt;C-H&gt; Split ResizingVim’s defaults are useful for changing split shapes: 12345678\"Max out the height of the current splitctrl + w _\"Max out the width of the current splitctrl + w |\"Normalize all split sizes, which is very handy when resizing terminalctrl + w = Split Manipulation12345678\"Swap top/bottom or left/right splitCtrl+W R\"Break out current window into a new tabviewCtrl+W T\"Close every window in the current tabview but the current oneCtrl+W o That’s it.","link":"/2019/08/18/VIM-split-ticks-and-tips/"},{"title":"pFsense tcp offloading in kvm virtio","text":"如果在KVM中创建 pFsense 实例用于 OpenStack 虚拟机的三层网关，在虚拟机之间的私有网络或者虚拟机对外网络会出现问题。例如虚拟机的DNS 解析（UDP Port 53）不正常，内部可以ping通dns 服务器，也可以ping通目标机器，但就是DNS解析无法工作。 在排除了是虚拟机的Egress规则以及pFsense中的防火墙问题外，那么可能就是由于虚拟网卡（VirtIO）以及物理网卡的问题。通过勾选pFsense中的’Disable hardware checksum offload’和’Disable hardware TCP segmentation offload’ 可以解决问题。TCP Offloading 的功能可能在ssl termination 中会有用处，但我们只用到了pFsense 的路由及防火墙功能，应用上面的负载均衡，漏洞扫描等尚未涉及到，所以关闭该功能对虚拟机来说没有影响。 Ref: https://docs.netgate.com/pfsense/en/latest/hardware/troubleshooting-lost-traffic-or-disappearing-packets.html https://forum.netgate.com/topic/82332/new-sg2440-disable-hardware-tcp-segmentation-offload","link":"/2019/11/06/pfsense-tcp-offloading-kvm-virtio/"},{"title":"Jenkins-X-Development-101","text":"Jenkins-X is an awesome CI/CD tool that working best with Kubernetes, it is also the fundation of CI/CD as a Service company of CloudBees. All of the buzz words and the new techs adopted. GitOps, Istio, Multi-Cluster, Approval mechanism… But the project is still under developing, some features may not implemented or have minior issues need to be fixed. The article outlined the tasks need to hack the jenkins-x with your own requirment and environment. Prerequisite Resource Version Jenkins X A JX instance deployed with jx boot Docker Registry - Helm Chart Repository chartmuseum Golang Environment 1.11 or 1.12 and also forked the following repository into your own org https://github.com/jenkins-x/jenkins-x-boot-config https://github.com/jenkins-x/jenkins-x-platform https://github.com/jenkins-x/jenkins-x-versions the codebase repository that you’re trying to hack PersonaAssuming I want to make the Jenkins X works on the kube api 1.16+.So if you followed the docs to install jenkins X on a on premise kube cluster like k3s distribution v1.16.3-k3s.2, you should hit the issue like below: 123456789configmap/config-entrypoint createdconfigmap/config-logging createdunable to recognize &quot;/var/folders/62/l5t1s2lj17jd5nwlzw70st1r0000gn/T/helm-template-workdir-114960915/jenkins-x/output/namespaces/jx/env/charts/jenkins-x-platform/charts/chartmuseum/templates/part0-deployment.yaml&quot;: no matches for kind &quot;Deployment&quot; in version &quot;extensions/v1beta1&quot;unable to recognize &quot;/var/folders/62/l5t1s2lj17jd5nwlzw70st1r0000gn/T/helm-template-workdir-114960915/jenkins-x/output/namespaces/jx/env/charts/jenkins-x-platform/charts/heapster/templates/part0-deployment.yaml&quot;: no matches for kind &quot;Deployment&quot; in version &quot;extensions/v1beta1&quot;unable to recognize &quot;/var/folders/62/l5t1s2lj17jd5nwlzw70st1r0000gn/T/helm-template-workdir-114960915/jenkins-x/output/namespaces/jx/env/charts/jenkins-x-platform/charts/nexus/templates/part0-deployment.yaml&quot;: no matches for kind &quot;Deployment&quot; in version &quot;extensions/v1beta1&quot;unable to recognize &quot;/var/folders/62/l5t1s2lj17jd5nwlzw70st1r0000gn/T/helm-template-workdir-114960915/jenkins-x/output/namespaces/jx/env/charts/lighthouse/templates/part0-deployment.yaml&quot;: no matches for kind &quot;Deployment&quot; in version &quot;extensions/v1beta1&quot;unable to recognize &quot;/var/folders/62/l5t1s2lj17jd5nwlzw70st1r0000gn/T/helm-template-workdir-114960915/jenkins-x/output/namespaces/jx/env/charts/lighthouse/templates/part0-tide-deployment.yaml&quot;: no matches for kind &quot;Deployment&quot; in version &quot;extensions/v1beta1&quot;unable to recognize &quot;/var/folders/62/l5t1s2lj17jd5nwlzw70st1r0000gn/T/helm-template-workdir-114960915/jenkins-x/output/namespaces/jx/env/charts/tekton/templates/part0-controller.yaml&quot;: no matches for kind &quot;Deployment&quot; in version &quot;apps/v1beta1&quot;&apos;error: failed to interpret pipeline file jenkins-x.yml: failed to run &apos;/bin/sh -c jx step helm apply --boot --remote --name jenkins-x --provider-values-dir ../kubeProviders&apos; command in directory &apos;env&apos;, output: &apos; The logs above shows that there are issues in the helm charts: jenkins-platform, lighthouse and tekton, so let’s see what’s the issue it is and forked these repos. Let’s take lighthouse for example: https://github.com/jiangytcn/lighthouse/blob/master/charts/lighthouse/templates/deployment.yaml#L3 The deployment is using wrong resource version which not compatible with my kube cluster in https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/, so did with other repos. Ok, so we got the root cause of the problems and the fixes for that, how can I applied these changes to the jx environment ? HacksCause JX is using helm charts for the deployment, before we apply changes in it, let udpate the helm charts of problematic repos. For the development purpose, I setup the helm repository locally with chartmusem without auth. build customized charts 12345678$ tree . -L 1.├── Makefile├── jenkins-x-platform├── jenkins-x-versions├── lighthouse├── platform-config└── tekton After update the code in ligthouse, tekton upload the charts to local registry 12345678910111213141516171819202122232425262728293031323334353637CHART_REPO := $&#123;CHART_REPO&#125;NAME := $&#123;NAME&#125;OS := $(shell uname)init: helm init --client-onlysetup: init helm repo add jenkinsxio http://chartmuseum.jenkins-x.iobuild: clean setup helm dependency build $&#123;NAME&#125; helm lint $&#123;NAME&#125;clean: git checkout \"$&#123;NAME&#125;/Chart.yaml\" rm -rf $&#123;NAME&#125;/charts rm -rf $&#123;NAME&#125;/$&#123;NAME&#125;*.tgz rm -rf $&#123;NAME&#125;*.tgz rm -rf $&#123;NAME&#125;/requirements.lock curl -w '\\n' -X DELETE $(CHART_REPO)/api/charts/$&#123;NAME&#125;/$&#123;VERSION&#125;release: clean buildifeq ($(OS),Darwin) sed -i -e \"s/version:.*/version: $(VERSION)/\" \"$&#123;NAME&#125;/Chart.yaml\"else ifeq ($(OS),Linux) sed -i -e \"s/version:.*/version: $(VERSION)/\" $&#123;NAME&#125;/Chart.yamlelse exit -1endif helm package $&#123;NAME&#125; curl -w '\\n' --fail --data-binary \"@$(NAME)-$(VERSION).tgz\" $(CHART_REPO)/api/charts rm -rf $&#123;NAME&#125;*.tgz git checkout \"$&#123;NAME&#125;/Chart.yaml\"delete-from-chartmuseum: curl --fail -X DELETE $(CHART_REPO)/api/charts/$(NAME)/$(VERSION) Copy the content above to Makefile. Build chart separately and upload to registry with: 12345678cd tektonNAME=tekton VERSION=100.0.0 CHART_REPO=http://chartmuseum.int.caas.ystacks.com:8080 make -f ../Makefile releasecd lighthouse/chartsNAME=lighthouse VERSION=100.0.0 CHART_REPO=http://chartmuseum.int.caas.ystacks.com:8080 make -f ../../Makefile releasecd jenkins-x-platformNAME=jenkins-x-platform VERSION=100.0.0 CHART_REPO=http://chartmuseum.int.caas.ystacks.com:8080 make -f ../Makefile release ThenAdd your local helm charts registry to the jenkins-x-versions for searching the customized chartshttps://github.com/jiangytcn/jenkins-x-versions/blob/chore-compatible-with-1.16/charts/repositories.yml#L12and the version for other reposhttps://github.com/jiangytcn/jenkins-x-versions/blob/chore-compatible-with-1.16/charts/jenkins-x/lighthouse.yml#L2https://github.com/jiangytcn/jenkins-x-versions/blob/chore-compatible-with-1.16/charts/jenkins-x/tekton.yml#L2 after finished the change, bump a new tag for hack branch - v100.0.0 Deploy with the fix update the jx deployment file jx-requriement.ymlchange the section to your own forked url with valid ref123versionStream: ref: v100.0.0 url: https://github.com/jiangytcn/jenkins-x-versions.git and jx boot Reference:conceptscontributing Drop me messages for any issue and happy hacking :)","link":"/2020/01/01/Jenkins-X-Development-101/"},{"title":"Add route for VirtualBOX hostonly ifs","text":"最近在使用Mac上的VirtualBox 创建出的虚拟机做K8S相关的开发工作， 物理机有时重启后无法连接到虚拟机当中，ICMP拒绝， 但是在VM 内部以及VM间网络通信都是正常的。重启Mac后问题可以解决，但是不是解决问题之道，经过排查，发现Mac上的到虚拟机hostonly 网络的路由丢失，导致连接失败 查看当前物理机上的hostonly 网卡信息 123456789101112131415161718192021222324252627282930313233343536373839$ VBoxManage list hostonlyifsName: vboxnet0GUID: 786f6276-656e-4074-8000-0a0027000000DHCP: DisabledIPAddress: 192.168.50.1NetworkMask: 255.255.255.0IPV6Address:IPV6NetworkMaskPrefixLength: 0HardwareAddress: 0a:00:27:00:00:00MediumType: EthernetWireless: NoStatus: UpVBoxNetworkName: HostInterfaceNetworking-vboxnet0Name: vboxnet1GUID: 786f6276-656e-4174-8000-0a0027000001DHCP: DisabledIPAddress: 192.168.59.1NetworkMask: 255.255.255.0IPV6Address:IPV6NetworkMaskPrefixLength: 0HardwareAddress: 0a:00:27:00:00:01MediumType: EthernetWireless: NoStatus: DownVBoxNetworkName: HostInterfaceNetworking-vboxnet1Name: vboxnet2GUID: 786f6276-656e-4274-8000-0a0027000002DHCP: DisabledIPAddress: 192.168.99.1NetworkMask: 255.255.255.0IPV6Address:IPV6NetworkMaskPrefixLength: 0HardwareAddress: 0a:00:27:00:00:02MediumType: EthernetWireless: NoStatus: DownVBoxNetworkName: HostInterfaceNetworking-vboxnet2 Add dhcp serverVBoxManage dhcpserver modify --ifname vboxnet0 --ip 192.168.50.2 --netmask 255.255.255.0 --lowerip 192.168.50.100 --upperip 192.168.50.199 --enable 查看虚拟机网络网关的路由信息 123456789$ route get 192.168.50.1route to: 192.168.50.1destination: defaultmask: defaultgateway: 192.168.1.1interface: en0flags: &lt;UP,GATEWAY,DONE,STATIC,PRCLONING&gt;recvpipe sendpipe ssthresh rtt,msec rttvar hopcount mtu expire0 0 0 0 0 0 1500 0 可以看到此网络的网关走到了192.168.1.1 此地址为Mac机器的网关，所以到该虚拟网络的流量都会通过gw 出去，因此也就到达不了虚拟机内部 添加到虚拟网络地址段的路由 $ sudo route -nv add -net 192.168.50 -interface vboxnet0 12345u: inet 192.168.50.0; u: link vboxnet0:a.0.27.0.0.0; u: inet 255.255.255.0; RTM_ADD: Add Route: len 140, pid: 0, seq 1, errno 0, flags:&lt;UP,STATIC&gt;locks: inits:sockaddrs: &lt;DST,GATEWAY,NETMASK&gt;192.168.50.0 vboxnet0:a.0.27.0.0.0 255.255.255.0add net 192.168.50: gateway vboxnet0 $ route get 192.168.50.114route to: 192.168.50.114destination: 192.168.50.0mask: 255.255.255.0interface: vboxnet0flags: &lt;UP,DONE,CLONING,STATIC,PRCLONING&gt;recvpipe sendpipe ssthresh rtt,msec rttvar hopcount mtu expire0 0 0 0 0 0 1500 -438此时可以看到，到该虚拟网络的地址都通过vboxnet0 $ ping 192.168.50.115PING 192.168.50.115 (192.168.50.115): 56 data bytes64 bytes from 192.168.50.115: icmp_seq=0 ttl=64 time=0.287 ms^C— 192.168.50.115 ping statistics —1 packets transmitted, 1 packets received, 0.0% packet lossround-trip min/avg/max/stddev = 0.287/0.287/0.287/0.000 ms","link":"/2019/08/05/Add-route-for-VirtualBOX-hostonly-ifs/"},{"title":"Jenkins-X Technical Briefings (Pipeline)","text":"Jenkins-X is an awesome CI/CD tool that working best with Kubernetes, it is also the fundation of CI/CD as a Service offering of CloudBees company.Just as the functions it says, Continous Integration and Continous Delivery, but how Jenkins-X achieved that goal? what’s the mechanism behind of the tool? MethodJenkins-X uses the familiar objects that you would ordinarily use to ship the service; kaniko to build container images within kubernetes; helm charts skeleton to illustrate the deployment of the service; Tekton to assemble tasks into a pipeline; lighthouse(developed internally) or prow to handle the chatops activities. So that’s the most important parts of the Jenins-X. Here’s a typical tekton pipeline used for a golang application image building,packaging and delivery to kubernentes.For a given repository, there would be two pipelines involved The tekton pipeline and the related resources are kubernetes CRDs, including: PipelineResource, the only supported types are git and image a Task is the unit to completedly run a job, contains a series of task instance TaskRun the single task running instance, contains the full lifecyle of the task. Pipeline control multiple tasks which assembled into a pipeline template. PipelineRun an instance to run a spcific pipeline To make it simple, the Task and Pipeline are the templates to accomplish a goal, TaskRun and PipelineRun are the instance to execute these templates at the given time. From the diagram above, we can see that an application contians two pipelines: meta and release. The meta pipeline is used to create effective pipeline dynamatically with the build-pack of the application and in the last step to trigger that templated pipeline with application specific parameters. 12345678910111213141516171819202122$ tree . -L 2 .├── charts│ ├── awesome-go│ ├── golang│ └── preview├── curlloop.sh├── Dockerfile├── jenkins-x.yml├── main.go├── Makefile├── OWNERS├── OWNERS_ALIASES├── README.md├── skaffold.yaml└── watch.sh4 directories, 10 files$ cat jenkins-x.yml buildPack: go the jenkins-x.yml in application source code illustrate the build-pack for building. Since we’re using kubernetes based installation, all available buildpacks stored in github, you can create your own buildpack with unique requirement. Tekton Pipeline UsageLet’s make some simulations of the critical parts in Jenkins-X, the source code is the one used above.The workflow is 123clone repo build container image and pushing build helm release and uploading to local chartmuseum Prerequisites Image buildingwe need the the service account for accessing kubernetes and the credentials for image registry to store container images. create image registry 1234kubectl create secret docker-registry registrysecret \\ --docker-server=&lt;registry address&gt; \\ --docker-username=&lt;user name&gt; \\ --docker-password=&lt;password&gt; create git repository access token 123kubectl create secret generic git-credentials --type=kubernetes.io/basic-auth \\ --from-literal=username=&lt;username&gt; \\ --from-literal=password=&lt;apitoken&gt; create kubernetes service account k8s-sa.yaml 1234567apiVersion: v1kind: ServiceAccountmetadata: name: tekton-bot-sasecrets:- name: registysecret- name: git-credentials 1kubectl apply -f k8s-sa.yaml create the clusterrolebinding 123456789101112apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: tekton-bot-handsonroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: tekton-bot-sa namespace: jx create helm release registry chartmuseum 1docker run --restart=always -d -it -p 8080:8080 -e DEBUG=1 -e STORAGE=local -e STORAGE_LOCAL_ROOTDIR=/charts -v $PWD/chartmuseum-int:/charts chartmuseum/chartmuseum:latest Tekton Pipeline MiscPipelineResources123456789101112apiVersion: tekton.dev/v1alpha1kind: PipelineResourcemetadata: name: jiangytcn-awesome-go-master namespace: tekton-handsonspec: params: - name: revision value: v0.0.1 - name: url value: http://github.192.168.1.12.nip.io/jiangytcn/awesome-go.git type: git kubectl apply -f tekton-pipelineresource.yaml Task kubectl apply -f tekton-task-build.yaml Pipelinecreate pipeline that reference to task and hte pipelineresource created above kubectl apply -f tekton-pipeline.yaml PipelineRuncreate a pipeline instance with required parameters kubectl apply -f tekton-pipelinerun.yaml The pipeline won’t run until create the pipelinerun instance handson repository","link":"/2020/02/14/Jenkins-X-Technical-Briefings-Pipeline/"},{"title":"Use teamsql in CloudFoundry","text":"Most of cloudfoundry based platform have postgres service in their catalog, applications can use the credentials bounded to their running environment for accessing, but sometimes the administrator need to access the database instance for whatever reason, debugging or checking specific record. TeamPostgreSQL is a project of Webworks (webworks.dk), which is excellent for the webui based database management, using a single browser, user could manage the postgresql database. download release 123/usr/bin/curl -# http://cdn.webworks.dk/download/teampostgresql_multiplatform.zip \\ -o teampostgresql_multiplatform.zip &amp;&amp; \\unzip teampostgresql_multiplatform.zip &amp;&amp; rm -f teampostgresql_multiplatform.zip generate deploy manifest file for teampostgresqlThe application would use java buildpack for compiling and hosting, and need to specify JBP_CONFIG_JAVA_MAIN otherwise the http service won’t start correctly 12345678---applications:- name: psqlgui memory: 1G buildpack: https://github.com/cloudfoundry/java-buildpack.git host: psqlgui env: JBP_CONFIG_JAVA_MAIN: &apos;&#123;arguments: &quot;-cp WEB-INF/lib/log4j-1.2.17.jar-1.0.jar:WEB-INF/classes:WEB-INF/lib/* dbexplorer.TeamPostgreSQL $PORT&quot;&#125;&apos; Deployusing the following script to deploy application, before executing, need to obtain an account of cloudfoundry 1234567891011121314#!/bin/bashWORK_DIR=`dirname \"$&#123;0&#125;\"`RUN_DIR=$TMPDIRcf --version &gt; /dev/nullif [[ $? -gt 0 ]]; then echo \"CloudFoundry cli not installed\" exit 1ficf push -f $WORK_DIR/manifest.yml -p $RUN_DIR/teampostgresql/webapp After the deployment, the application could be access through webui and then configure database with credential of postgresql database instance","link":"/2019/08/05/use-teamsql-in-CloudFoundry/"},{"title":"OpenStack Nova 添加扩展API流程","text":"例子中涉及到SQLAlchemy 得相关操作，可以参考 [上一随笔] Openstack 中规定，扩展openstack得api有两种方式 创建新的WSGI 资源 扩展原有得WSGI资源得控制器（我得理解是，接受到API请求后，具体得响应逻辑） 这两种方式中，都要求写一个新的模块来声明控制器类去处理请求和实现扩展。 在一个API模块中，可以有一个或多个得资源和扩展控制器。 根据osapi_compute_extension 得配置， ExtensionManager 由nova/api/openstack/compute/contrib/ 下的init.py 文件加载标准的或者新的扩展。 所以扩展的api统一写在nova/api/openstack/compute/contrib/ 目录下 如本例子中得 nova/api/openstack/compute/contrib/documents.py 扩展API流程实现控制器，完成对资源的基本操作，如增删改查和其他一些用户自定义的RESTful资源操作； 实现一个extensions.ExtensionDescriptor的子类， 并实现get_resources 或者 get_controller_extensions，来建立新的资源或扩展资源控制器（即改写原有的业务逻辑）。具体实现哪个方法这取决于是否要修改原有的RESTFul业务逻辑， 或者说两个功能都需要； 将控制器和扩展的资源类，写如新创建的的资源中； 规范是资源类是模块（问家名）首字母大写，这样做的目的是使nova.api.openstack.extensions.load_standard_extensions这个类能够是别该新资源，并予以加载； 当添加新的资源（extensions.ResourceExtension的子类）的时候，如果想要去除掉 {tenent_id} 链接，则需要编写自定义的路由访问规则（本例子中没有涉及） documents.py 实现1 # vim: tabstop=4 shiftwidth=4 softtabstop=4 2 3 # Author: Yitao Jiang 4 # Email: willierjyt@gmail.com 5 6 import webob 7 from webob import exc 8 9 from nova import db 10 from nova import exception 11 from nova.api.openstack import extensions 12 authorize = extensions.extension_authorizer('compute', 'documents') 13 14 # 请求控制器， 即处理对资源的请求，予以响应 15 class DocumentsController(): 16 \"\"\"the Documents API Controller declearation\"\"\" 17 18 def index(self, req): 19 import pdb; pdb.set_trace() 20 documents = {} 21 context = req.environ['nova.context'] 22 authorize(context) 23 24 documents[\"key\"] = \"helloworld\" 25 return documents 26 27 def create(self, req): 28 documents = {} 29 context = req.environ['nova.context'] 30 authorize(context) 31 32 documents[\"key\"] = \"helloworld\" 33 return documents 34 35 def show(self, req, id): 37 documents = {} 38 context = req.environ['nova.context'] 39 authorize(context) 40 41 try: 42 document = db.document_get(context, id) 43 except : 44 raise webob.exc.HTTPNotFound(explanation=\"Document not found\") 45 46 documents[\"document\"] = document 47 return documents 48 49 def update(self, req): 50 documents = {} 51 context = req.environ['nova.context'] 52 authorize(context) 53 54 documents[\"key\"] = \"helloworld\" 55 return documents 56 57 def delete(self, req, id): 58 return webob.Response(status_int=202) 59 # 根据命名规范， 模块（python源文件）中的类名是模块名的首字母大写 60 class Documents(extensions.ExtensionDescriptor): 61 \"\"\"Documents ExtensionDescriptor implementation\"\"\" 62 63 name = \"documents\" 64 alias = \"os-documents\" 65 namespace = \"www.www.com\" 66 updated = \"2013-05-19T00:00:00+00:00\" 67 68 def get_resources(self): 69 \"\"\"register the new Documents Restful resource\"\"\" 70 71 resources = [extensions.ResourceExtension('os-documents', 72 DocumentsController()) 73 ] 74 75 return resources 在之后可以由以下几种方式来操作documents 资源 GET v2/{tenant_id}/ os-documents POST v2/{tenant_id}/ os-documents GET v2/{tenant_id}/ os-documents/{document_id} PUT v2/{tenant_id}/ os-documents/{document_id} DELETE v2/{tenant_id}/ os-documents/{document_id} 扩展api时所修改的文件 1 nova/db/api.py 2 nova/db/sqlalchemy/api.py 3 nova/db/sqlalchemy/models.py nova/db/api.py 文件内容 数据操作API提供的方法，由Nova API 根据请求进行相应的操作， 由上面的请求控制器进行调用 1 def document_get(context, document_id): 2 \"\"\"Get a document or raise if it does not exist.\"\"\" 3 return IMPL.document_get(context, document_id) nova/db/sqlalchemy/api.py 文件内容 # 完成通过由SQLAlchemy操作数据库 1 @require_admin_context 2 def document_get(context, document_id): 3 4 session = get_session() 5 with session.begin(): 6 query = model_query(context, models.Document, session=session, read_deleted=\"yes\").filter_by(id=document_id) 7 8 result = query.first() 9 10 if not result or not query: 11 raise Exception() 12 13 return result SQLAlchemy 中定义的资源 nova/db/sqlalchemy/models.py（具体使用见上一 篇日志） 1 class Document(BASE, NovaBase): 2 \"\"\"Represents a document of customized extension.\"\"\" 3 4 __tablename__ = 'documents' 5 id = Column(Integer, primary_key=True) 6 title = Column(String(255)) 至此，添加添加新的nova API功能完成,重起api服务 调用 curl -v -X GET -H ‘X-Auth-Token: 8e5971b3ce0a4f039b895681e7c29361’ http://127.0.0.1:8774/v2/9b5903dd2d3443d8bb75ddffac27239a/extensions/os-documents | python -mjson.tool命令，可以看到返回，扩展添加成功 1 { 2 \"extension\": { 3 \"alias\": \"os-documents\", 4 \"description\": \"Documents ExtensionDescriptor implementation\", 5 \"links\": [], 6 \"name\": \"documents\", 7 \"namespace\": \"www.www.com\", 8 \"updated\": \"2013-05-19T00:00:00+00:00\" 9 } 10 } 数据库添加表documents， 结构如下 mysql> desc documents; +------------+--------------+------+-----+---------+----------------+ | Field | Type | Null | Key | Default | Extra | +------------+--------------+------+-----+---------+----------------+ | id | int(11) | NO | PRI | NULL | auto_increment | | title | varchar(255) | NO | | NULL | | | created_at | datetime | YES | | NULL | | | updated_at | datetime | YES | | NULL | | | deleted_at | datetime | YES | | NULL | | | deleted | int(11) | YES | | NULL | | +------------+--------------+------+-----+---------+----------------+ 6 rows in set (0.05 sec) mysql> select * from documents; +----+----------------+------------+------------+------------+---------+ | id | title | created_at | updated_at | deleted_at | deleted | +----+----------------+------------+------------+------------+---------+ | 1 | abcdefgiifeife | NULL | NULL | NULL | NULL | | 2 | 1qaz2wsx | NULL | NULL | NULL | NULL | +----+----------------+------------+------------+------------+---------+ 2 rows in set (0.03 sec) 1 curl -X GET -H 'X-Auth-Token: 8e5971b3ce0a4f039b895681e7c29361' http://127.0.0.1:8774/v2/9b5903dd2d3443d8bb75ddffac27239a/os-documents/1 | python -mjson.tool 2 返回结果 3 { 4 \"document\": { 5 \"created_at\": null, 6 \"deleted\": null, 7 \"deleted_at\": null, 8 \"id\": 1, 9 \"title\": \"abcdefgiifeife\", 10 \"updated_at\": null 11 } 12 } 至此新添加的资源，API 扩展成功， 可以在此基础上进行进一步的修改，完成需求 参考文档： Adding a Method to the OpenStack API","link":"/2014/05/17/openstack-nova-adding-plugin/"},{"title":"Cloudstack Debugging SystemVm agents","text":"##Cloudstack Debug a live agentlogin into ssvm, either console proxy or ssh(port 3922)kill all the processes named as(run.sh/_run.sh, and java) cd /usr/local/cloud/systemvm add parameters “-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=8787” after “java “ in the last line of _run.sh ./run.sh, the java agent will start, with debug port 8787 is listened on. allow port 8787 in ssvm, “iptables -I INPUT -i eth1 -p tcp -m state –state NEW -m tcp –dport 8787 -j ACCEPT”, either eth1 or eth2 is ok. Log in into ssvm - Log into the hypervisor and then type the following command “ssh -i /opt/xensource/bin/id_rsa –p 3922 root@privateIP_or_LinkLocalIpofSSVM”, or “ssh -i /root/.ssh/id_rsa.cloud -p 3922 root@LinkLocal” on XenServer. Private ip in case of vmware and linklocal in case Xenserver. Vmware do not have link local ip. SSVM can be accessed from Management server using private ip address .Ex: ssh -i /var/cloudstack/management/.ssh/id_rsa -p 3922 root@ Log in into ssvm - Log into the hypervisor and then type the following command “ssh -i /opt/xensource/bin/id_rsa –p 3922 root@privateIP_or_LinkLocalIpofSSVM”, or “ssh -i /root/.ssh/id_rsa.cloud -p 3922 root@LinkLocal” on XenServer. Private ip in case of vmware and linklocal in case Xenserver. Vmware do not have link local ip. SSVM can be accessed from Management server using private ip address .Ex: ssh -i /var/cloudstack/management/.ssh/id_rsa -p 3922 root@ SSVM health check - Run the following script inside ssvm /usr/local/cloud/systemvm/ssvm-check.sh It checks for 1. connectivity with DNS server 2. resolving of domain names 3. status of secondary storage 4. ability to write to secondary storage 5. connectivity with management server at port 8250 and status of java process. NOTE - If you dont find the health check script then go to #9. Most probably your ssvm didnt get patched right. Template not ready / not available when creating an instance - Many a times the SSVM is running but still the templates do not show as ready or to say templates are not available when creating an instance. Run the health check script above and diagnose. The most probable reason reason is that the agent running on SSVM hasn’t been able to connect with MS which could also be validated by checking the host table in DB. select * from host where type like ‘SecondaryStorageVM’. If the status shows as Alert then definitely that is the reason. There could be a number of reasons for the agent not being able to connect with MS. Below three could be one of them. Check whether port 8250 is open on MS and there is no firewall rule. This is the port on which the agent and MS communication happens. Check whether the SSVM is trying to connect to the right ip of MS. If it is incorrect it could be due to the wrong ip being set in the global settings (configuration table) for ‘host’ in MS. Change that, restart MS and SSVM and see if it solves the issue. Check the agent status on SSVM- See if the agent is running by typing “service cloud status” in SSVM. Try to run it and see if that’s successful or changes the alert status. To check the state of templates whether is has downloaded or there is an error - Log into DB and check table template_host_ref and observe the download_state and error_string. Templates stuck in download in progress - Either stop and then start the SSVM. Or, run service cloud restart on the SSVM. You can also restart MS. This would trigger template sync which essentially will try and resume such stuck templates or redo the download of erred out templates. Whenever there is a handshake between MS and the agent running on the SSVM there is a template sync which syncs the template status on the MS DB and the template’s physical Location and triggers the download if its not complete or retries in case there was some error. (This handshake happens on ssvm re/start, agent re/start and MS re/start)Connection refused as the status for the template - Check whether the config parameter “secstorage.allowed.internal.sites” has been set to allow the internal n/w URL’s. Retrying the download of templates - Refer #5 above ###no route to hostThis error often implies your firewall blocks the traffic, check iptable rules in SSVM then host then physical firewall.###SSVM agent not runningYou see error something like below. Most probably your systemvm didnt get patched with the agent specific code. This patching happens through the ISO - something like systemvm***.iso. Check the size and location of the iso and whether you are not using the old version iso. For XS its on the host so grep for it and for vmware its on secondary storage. Do also check that if its vmware that you built your ssvm using noredist flag for mvn command.2013-12-20 13:35:12,954 DEBUG [cloud.utils.ProcessUtil] (main:null) Execution is successful.2013-12-20 13:35:12,960 ERROR [cloud.agent.AgentShell] (main:null) Unable to start agent: Resource class not found: com.cloud.storage.resource.PremiumSecondaryStorageResource due to: java.lang.ClassNotFoundException: com.cloud.storage.resource.PremiumSecondaryStorageResourceDont see any health check script on SSVM or the agent running - The issue for sure is as in #9 above. For vmware setup I saw that because the systemvm.iso size wasnt right.SSVM Logs - /var/log/cloud/cloud.log*ERROR: Java process not running. Try restarting the SSVM - *It looks like the scripts/java binaries have not been copied into the SSVM ? See this thread http://markmail.org/thread/niu2qwsdydkuh2f for how it is supposed to work (response from Alex)###Available secondary storage space is lowshows only ~2 gb. Generally this is because the secondary storage is not mounted correctly. Run the step 2 above to verify that. One of the reasons could be “It’s due to IP access-list settings which was turned on by default on NAS4Free. Have allowed the SSVM’s IP address to mount to the NAS and it’s now working fine.”SSVM nics - SSVM basically has four nics, they are: eth0: link local nic used for ssh login from host eth1: private nic used as management interface between mgmt server and SSVM eth2: public nic used as interface that can reach outside internet eth3: storage nic used as interface to access secondary storage share like NFS CloudStack sets route for each nic, however, the most important route ‘default’ is set to public nic which is eth2. That means a healthy SSVM should have default route like(by command ‘ip route’):default via public_gateway_ip_address dev eth2 this also implies communication between SSVMs happen thru public nic even both SSVMs are in the same private subnet. SSVM templates physical location - find the mount point by typing command “mount” . Go to the directory and under template/tmpl you will find all the templates. SSVM Apache server - For 2.2 onwards the system vms are debian based. Type “service apache2 status” to find the status. Apache root is at /www/html/Run script of java process /usr/local/cloud/systemvm/run.shIncreasing log level - 1) Edit the file /usr/local/cloud/systemvm/conf/log4j-cloud.xml 2) For the log file cloud.log change the threshold to info: to 3) Change com.cloud to INFO: If you’re not getting sufficient logging, you can also try setting it to DEBUG. Multiple secondary storages feature has been added since some time now. The private templates are copied to one of the secondary storages and public to all of them for higher availability.All the public templates will be replicated to all the secondary storage for redundancy but private templates and uploaded volumes would be kept in one of the randomly chosen secondary storage. Snapshots are randomly copied to one of the secondary storage (the chain of incremental snapshots are kept on the same secondary storage.) Currently these algorithms are hardcoded and there is no way to tweak it. In case you need flexibility please open an enhancement for the same. During template sync the consistency is again checked and download triggered for templates keeping in mind the algorithm above.SSVM and CPVM do not have agents running AND SSVM health check runs fine. cloud.log on SSVM/CPVM keeps showing the following message:2014-06-16 08:08:01,108 INFO [utils.nio.NioClient] (Agent-Selector:null) Connecting to 10.102.192.247:82502014-06-16 08:08:01,872 ERROR [utils.nio.NioConnection] (Agent-Selector:null) Unable to initialize the threads.java.io.IOException: SSL: Fail to init SSL! java.io.IOException: Connection closed with -1 on reading size.at com.cloud.utils.nio.NioClient.init(NioClient.java:84)at com.cloud.utils.nio.NioConnection.run(NioConnection.java:108)at java.lang.Thread.run(Thread.java:701) ###Solution : rm ./client/target/cloud-client-ui-4.3.0.0/WEB-INF/classes/cloudmanagementserver.keystore ./client/target/conf/cloudmanagementserver.keystore ./client/target/generated-webapp/WEB-INF/classes/cloudmanagementserver.keystore remove root entry from cloud.keystore; remove ssl.keystore from cloud.configuration where description like ‘%key%’; restart MS agent in ssvm Download Complete 100% but getting error like this Failed post download script: /usr/sbin/vhd-utilvhd tool check /mnt/SecStorage/33e2e9f5/template/tmpl/345/447/dnld1469110483936142751tmp_ failed - Many reasons for this but amongst them are wrong OS selection, vhd corruption. Test this in the lab by copying the template to one of the hosts then on that host run vhd-util check -n filename.vhd vhd-util scan filename.vhd SSVM RAM - Set the param secstorage.vm.ram.size to in change the ram size of the vm. Default in the code is 256. For each secondary storage there is a corresponding row created in the host table. HTTP Server returned 403 (expected 200 OK) - For copy templates. Try to see the first log for this template initiation ? It should be logged with DownloadCommand and should have the url of the source ssvm’s template. Then you can try going to the destination SSVM and try downloading that url. See what issues you get. I would also check the iptable rules to see if the destination ssvm is blocked from accessing the source ssvm and also if there is any .htaccess file in the apache directories forbidding the download of template One of the problems as was as follows.*The problem is that we’re using basic networking &amp; have the private network setup with the same gateway &amp; subnet as the public network. When the storage VM comes up the public network gets setup first but then when the private network comes up on eth2 it clobbers the gateway &amp; sets it to the eth2 interface. So when the copy is initiated between the storage VMs it happens across the private network but the /var/www/html/copy/.htaccess file only allows the public IP of the other SSVM, thus the 403 errors.","link":"/2014/09/03/cloudstack-debugging-systemvm-agents/"},{"title":"cloud storage swift ceph","text":"###概述许多人对对象存储与像ISCSI、FC这类块存储混同，但是他们之间存在很大的差别。像fc这类的块存储只能提供块设备如系统中的sdb，对象存储只能通过特殊的客户端来访问，像百度网这一类 块存储对于云环境下是重要的一部分，主要用于存储虚拟机的镜像文件或者存储用户的文件，包括所有进行备份的数据，文件，图像等。对象存储的主要优势在于比企业级的商业存储更加的节约，并且保证规模扩展性和数据的冗余. ###Openstack Swift###软件架构 OpenStack 对象存储(Swift)利用标准服务器组建集群，提供了冗余的，可扩展的分布式对象存储。分布式意味着数据的每一分份在集群中的存储节点上复制。复制的份数是可配置的但是对于生产环境最少是3份。 Swift 中的数据可以通过REST接口访问，根据需要可以对存储的数据进行相应的操作。每一个对象的访问路径包含三个元素：/account/container/objectobject存储了用户的实际的输入的数据 Accounts and containers提供了组织对象的方式，不允许嵌套的accounts 和 containers。 Swift 软件以组件的方式进行开发，包括account servers, container servers, 和object servers 除此之外，proxy server哟用于接受用户的api请求Account servers 对账户提供container listings的操作 Container servers 对于一个给定的容器提供object listings 的操作 Object servers返回数据本身 ###Rings 由于用户的数据在集群中分布，所以非常的有必要记录数据存放的位置。 Swift通过维护一个叫做_rings_的数据结构来完成数据的定位操作。Rings 在集群中的各个节点复制包括存储节点以及代理节点，这种方式使得swift避免了大部分存储系统依赖于集中单一的meta date服务器的弊端。由于ring文件存储的是集群中node的关系，而不是一个集中的数据map，所以在存储或者删除object是，不需要更新ring文件.这样有益于IO操作，极大程度的减少了访问的带来的不便 对于acount数据库、container数据库、和单独的object都有独立的rings文件，但是都已相同的方式进行工作。简单来说就是，对于一个给定的Account，container，或者object，ring返回的是它在物理存储节点上的位置，从技术的角度来说，这一过程包括一致性hash算法。在mirantis上有对ring工作原理的相关介绍 under-hood-of-swift-ring 和 openstack-swift-consistency-analysis. ###Proxy Server代理服务器暴露出public API并且接受对存储实体的请求。对于每一个请求，代理节点都会通过ring文件查找account，container以及object所在的物理位置。根据位置将请求转发.Objects 在代理节点和客户端间直接的以流的的方式进行传递，并且没有缓存 ###Object server这是一个简单的blob的存储服务器，可以存储数据、查询、删除数据。对象以二进制文件的方式在存储节点中，对象的metadata信息存储在xattrs（file’s extended attributes）上，这就要求存储object的节点必须支持xattrs 每一个object利用对象名的hash值以及操作的时间戳来进行存储路径的存储，最新的写操作会在记录中的最新位置（包括在分布式的场景下，创建的请求需要全局的同步时钟）以保证响应对象的最新的一个版本。删除操作也会记录为文件的一个版本，这就保证了已经删除的对象在集群间正确的复制，老的版本不会出现在用户的请求中 ###Container serverContainer 服务器用于查询object信息（listings）。并不知道具体的object的位置，但是对于一个给定的container可以查询到其包含的objects。listings 数据存储在sqlite3数据库中，并且向object一样，在集群间复制。象object总数、container的存储使用情况的统计值都会记录下来. 在所服务的节点上，会有一个特殊的进程swift-container-updater 不间断的想container数据库中填充数据，在一个container中的数据变化时，并对其数据库进行更新。通过ring来定位需要更新的account。 ###Account server与container server雷系，但是是处理container listings 的操作. ###Features and functions Replication: The number of object copies that can be configured manually. Object upload is a synchronous process: The proxy server returns a “201 Created” HTTP code only if more than half the replicas are written. Integration with OpenStack identity service (Keystone): Accounts are mapped to tenants. Auditing objects consistency: the md5 sum of an object on the file system compared to its metadata stored in xattrs. Container synchronization: This makes it possible to synchronize containers across multiple data centers. Handoff mechanism: It makes it possible to use an additional node to keep a replica in case of failure. If the object is more than 5 Gb, it has to be split: These parts are stored as separate objects and could be read simultaneously. ##Ceph Ceph is a distributed network storage with distributed metadata management and POSIX semantics. The Ceph object store can be accessed with a number of clients, including a dedicated cmdline tool, FUSE, and Amazon S3 clients (through a compatibility layer, called “S3 Gateway“). Ceph is highly modular – different sets of features are provided by different components which one can mix and match. Specifically, for object store accessible via s3 API it is enough to run three of them: object server, monitori ###Monitor serverceph-mon is a lightweight daemon that provides a consensus for distributed decision-making in a Ceph cluster. It also is the initial point of contact for new clients, and will hand out information about the topology of the cluster. Normally there would be three ceph-mon daemons, on three separate physical machines, isolated from each other; for example, in different racks or rows. ###Object serverThe actual data put onto Ceph is stored on top of a cluster storage engine called RADOS, deployed on a set of storage nodes. ceph-osd is the storage daemon that runs on every storage node (object server) in the Ceph cluster. ceph-osd contacts ceph-mon for cluster membership. Its main goal is to service object read/write/etc. requests from clients, It also peers with other ceph-osds for data replication. The data model is fairly simple at this level. There are multiple named pools, and within each pool there are named objects, in a flat namespace (no directories). Each object has both data and metadata. The data for an object is a single, potentially big, series of bytes. The metadata is an unordered set of key-value pairs. Ceph filesystem uses metadata to store file owner, etc. Underneath, ceph-osd stores the data on a local filesystem. We recommend Btrfs, but any POSIX filesystem that has extended attributes should work. ###CRUSH algorithmWhile Swift uses rings (md5 hash range mapping against sets of storage nodes) for consistent data distribution and lookup, Ceph uses an algorithm called CRUSH for this. In short, CRUSH is an algorithm that can calculate the physical location of data in Ceph, given the object name, cluster map and CRUSH rules as input. CRUSH describes the storage cluster in a hierarchy that reflects its physical organization, and thus can also ensure proper data replication on top of physical hardware. Also CRUSH allows data placement to be controlled by policy, which allows CRUSH to adapt to changes in the cluster membership. ###Rados Gatewayradosgw is a FastCGI service that provides a RESTful HTTP API to store objects and metadata on the Ceph cluster. ###Features and functions Partial or complete reads and writes Snapshots Atomic transactions with features like append, truncate, and clone range Object level key-value mappings Object replicas management Aggregation of objects (series of objects) into a group, and mapping the group to a series of OSDs Authentication with shared secret keys: Both the client and the monitor cluster- have a copy of the client’s secret key Compatibility with S3/Swift API ###Feature summary Swift Ceph Replication Yes Yes Max. obj.size 5gb(bigger objects segmented) Unlimited Multi DCinstallation Yes (replication on the container level only,but a blueprint proposed for full inter dc replication) No (demands asynchronous eventual consistency replication, which Ceph does not yet support) Integration/w Opentsack Yes Partial(lack of Keystone support) Replicasmanagement No Yes Writingalgorithm Synchronous Synchronous Amazon S3 compatible API Yes Yes Data placement method Ring (static mapping structure) CRUSH (algorithm) ###参考文章 http://www.mirantis.com/blog/object-storage-openstack-cloud-swift-ceph","link":"/2014/06/03/cloud-storage-swift-ceph/"},{"title":"Kubernetes In Kubernetes the Easy Way","text":"If you are planing to adopt CloudNative way to ship your products in an efficient way, then Kubernetes might be a good choice to mondernize IT.After serveral years growth, the product went into an stable state and adpoted by large number of companies in Production environments. All major cloud prvoider players offerred their own kubernretes service with their own addons. Kelsey Hightower wrote an good article Kubernetes-the-hard-way to help you better understanding the tasks needed to deploy a workable kubernetes cluster. The minimal tasks needed are: container engine installation networking plugin installation certificate management and distribution storage backend management components configuration, apiserver, controller, scheduller, … More steps would be added to the the configureation pipelines as more features needed. To expose your service can be accessed publicly, you need to deploy a ingress service, to host a stateful service, for example, for a 3 tiers architecture, you need to deploy a database in the cluster, then cames the CSI compoennts. So, how can I get my own kubernetes based platform in an easy and efficient way ? how can I have multiple clusters with mimimal resources needed ? and how to keep up with the relases provided by the community to address the CVE, Bug Fix or new feature needed ? There are so many deployment tools around the kubenetes, and from CNCF there’re couple of certified installers to help your spin up and manage kuberntes cluster. From all of these tools, the uniqueness of Gardener is that the kubernets can be managed by kubernetes, fully utilized the benefit of kubernetes, CustomResource, Deployment, Ingress, etc want to upgrade a new version of kubernetes cluster? that’s easy, just use the webgui or with kubectl same as manage normal kube resource; want to reduce the resources occupied by the control plane of kubernetes? that’s easy, all end user kubernets cluster’s control plane would be running as pod; want to have a consistent deployment and fine gained control accross your different kubernets? that’s easy, gardener using Terraform and couple extensions to help you manage the resource deployed and configured; no human interaction; So, how to setup in my environment ?here’s my environment and all components setting, basically, you need to have an workable IaaS, a existed kubernetes cluster to host all gardener components, A DNS service to manage the records created for ingress services, and load balancer service which can be used for your kubernetes cloud provider.Lastly, need to have an object storeage to store the backups for resilence. Compoents Version Note IaaS - OpenStack stable/rocky The OpenStack IaaS is setup in my home lab environment backed by a 64G E5-2660 V2 DIY server, installed and configured with openstack-ansible DNS - OpenStack-Designate stable/rocky along side the designate, hosted CoreDNS as a DNS proxy, this is optional, just for debugging DNS related issue and to answer the requests outside the openstack environment LbaaS - OpenStack Haproxy based stable/rocky Base Cluster – K8S 1.16.2 Base K8S Cluster is setup with kubespray and my patch for fixing the etcd issue, my PR is not merged by community yet, so apply it if you had same issue Gardener - garden-setup 1.17.0 Before start the journey to expore Gardener, couples things need to configured correctly otherwise you will be failed to install gardener. The base kubernetes cluster network CNI configuration.If your’re using calico as the network cni for the undercloud cluster, the mtu need to be set correctly, otherwise the gardener components won’t connecte with each other correctly. choose approiate value per the guidance from calico networking MTU Configuration based on your own environment. For me, the cluster setup with IPIP mode and no jumboframe configured in my switch, so its 1430 the images can be used in gardener.After couple hours debugging and supported by gardener community, only a subset images supported by the installation miscs in Gardener, I only tried CoreOS and Ubuntu, gardener itself supported other os distributions, but no judgement about the compatibilities of other oses. OS Version Compatible Ubuntu 18.04 Y Ubuntu 16.04 X CoreOS 2135.6.0 Y CoreOS 2023.5.0 Y the image used for kubernetes worker nodes need to be uploaded to IaaS before the gardener installation start, it won’t download the image for you OpenStack API Access reachable from your kube cluster.Gardener using terrafrom to create resources on demand in your OpenStack environment, so need to make your IaaS reachable from the pods running in the kubernete cluster, dns, routing, etc. Installation under OpenStackThe installation steps is easy, providing the configuration file named acre.yml with correct setting along side a single command, that’s it! You can follow this guide to start the deployment, the processes are running under docker container. If you want to run that under your desktop to see how the majic happened, get the necessary tools from SOW Dockerfile and configure PATH correctly. The basic steps are: get your base kubernetes kubeconfig file configure acre.yaml correctly verify the configuration by sow order -A deploy the gardener by sow deploy -A access the gardener builtin ui by sow url 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182credentials: &amp;openstack username: admin password: ae226d1f8b27c60b31088 tenantName: admin domainName: Default userDomainName: Default authURL: https://api.int.ystacks.com:5000/v3 region: RegionOne bucketName: gardenerlandscape: name: overcloud-caas-ystacks-com domain: overcloud.caas.ystacks.com cluster: kubeconfig: ./kubeconfig domain: overcloud.caas.ystacks.com iaas: openstack networks: pods: 10.44.0.0/15 nodes: 10.132.0.0/16 services: 10.233.10.0/24 # change to under actual undercloud service ip cidr iaas: - name: openstack type: openstack mode: seed region: RegionOne zones: - nova credentials: *openstack floatingPools: - name: public loadBalancerProviders: - name: haproxy extensionConfig: machineImages: - cloudProfiles: - image: coreos-2135.6.0 name: openstack # A name: coreos # B version: 2135.6.0 # C - cloudProfiles: - image: ubuntu-18.04 name: openstack # A name: ubuntu # B version: &quot;18.04&quot; # C machineImages: - name: coreos versions: - version: 2135.6.0 - name: openstack versions: - version: &quot;18.04&quot; machineTypes: - name: 2C4G50G cpu: &quot;2&quot; gpu: &quot;0&quot; memory: 4Gi usable: true storage: class: default type: default size: 50Gi - name: 4C8G50G cpu: &quot;4&quot; gpu: &quot;0&quot; memory: 8Gi usable: true storage: class: default type: default size: 50Gi etcd: backup: type: swift credentials: *openstack dns: type: openstack-designate credentials: *openstack identity: users: - email: jiangyt.cn@gmail.com username: jiangyt.cn@gmail.com password: password After deploy you gardener cluster, accessing sow url to visit gardener dashboard and login with the user provided in identity above Note:created two clusters by gardener with different kubernetes versions Useful CommandsEdit Machine Image and Type on the flyThe machine image versions and types are stored as CR in the seed kube-apiserver, so targetting the seed kube virtual cluster with exported kubeconfig file to edit resources kubectl –kubeconfig export/kube-apiserver/kubeconfig edit controllerregistration.core.gardener.cloud/provider-openstack kubectl –kubeconfig export/kube-apiserver/kubeconfig edit cloudprofiles/openstack","link":"/2019/12/05/Kubernetes-In-Kubernetes-the-Easy-Way/"}],"tags":[{"name":"CloudStack","slug":"CloudStack","link":"/tags/CloudStack/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"XenServer","slug":"XenServer","link":"/tags/XenServer/"},{"name":"neutron","slug":"neutron","link":"/tags/neutron/"},{"name":"Network","slug":"Network","link":"/tags/Network/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"system","slug":"system","link":"/tags/system/"},{"name":"KVM","slug":"KVM","link":"/tags/KVM/"},{"name":"Virtualization","slug":"Virtualization","link":"/tags/Virtualization/"},{"name":"OpenStack","slug":"OpenStack","link":"/tags/OpenStack/"},{"name":"CI/CD","slug":"CI-CD","link":"/tags/CI-CD/"},{"name":"Kubernetes","slug":"Kubernetes","link":"/tags/Kubernetes/"},{"name":"CloudFoundry","slug":"CloudFoundry","link":"/tags/CloudFoundry/"},{"name":"CloudNative","slug":"CloudNative","link":"/tags/CloudNative/"},{"name":"MultiCloud","slug":"MultiCloud","link":"/tags/MultiCloud/"}],"categories":[{"name":"Cloudstack","slug":"Cloudstack","link":"/categories/Cloudstack/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"Xenserver","slug":"Xenserver","link":"/categories/Xenserver/"},{"name":"openstack","slug":"openstack","link":"/categories/openstack/"},{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"Git","slug":"Git","link":"/categories/Git/"},{"name":"cloudstack","slug":"cloudstack","link":"/categories/cloudstack/"},{"name":"sysadmin","slug":"sysadmin","link":"/categories/sysadmin/"},{"name":"Storage","slug":"Storage","link":"/categories/Storage/"}]}